{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "200f515cb4214f4cb520d8f8bc1f36a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1d78a8564d4945548183e41c7f949e1a",
              "IPY_MODEL_f0989fb473e74cb8ac82b73b1cd9de6c",
              "IPY_MODEL_e14153bfbaf84c79ba274c32abcc775e"
            ],
            "layout": "IPY_MODEL_bf0322956b4c41e5aaecae47c2724374"
          }
        },
        "1d78a8564d4945548183e41c7f949e1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9757b998754f4e8398cf336c6dcfa975",
            "placeholder": "​",
            "style": "IPY_MODEL_740cc90c8dc241a28a497770bae1e0a7",
            "value": "Epoch:   0%"
          }
        },
        "f0989fb473e74cb8ac82b73b1cd9de6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_286953f56dbe49ac852b3dac4d34d4eb",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c50e8b171f4a4e41baad5bf8a79da122",
            "value": 0
          }
        },
        "e14153bfbaf84c79ba274c32abcc775e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f49b866d0f9476995b6f87aabc04cdd",
            "placeholder": "​",
            "style": "IPY_MODEL_a1d6550c4abe4002a7eed9b0d30be92d",
            "value": " 0/100 [00:04&lt;?, ?it/s]"
          }
        },
        "bf0322956b4c41e5aaecae47c2724374": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9757b998754f4e8398cf336c6dcfa975": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "740cc90c8dc241a28a497770bae1e0a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "286953f56dbe49ac852b3dac4d34d4eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c50e8b171f4a4e41baad5bf8a79da122": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6f49b866d0f9476995b6f87aabc04cdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1d6550c4abe4002a7eed9b0d30be92d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e56398a656924094925ed6da9639af90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0d612e88990e4df280014354e024285a",
              "IPY_MODEL_68c4cf61318943cb88bc155dfe8c6b89",
              "IPY_MODEL_66be6ae60f464dd6b1fe82740b45ae0d"
            ],
            "layout": "IPY_MODEL_8c88a1a25d79443c83db625da7a7ecb8"
          }
        },
        "0d612e88990e4df280014354e024285a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0130f6e35418428aad81460152ce3eb6",
            "placeholder": "​",
            "style": "IPY_MODEL_78eed74360f841ed9a89db3afeaa8577",
            "value": "Batch:   4%"
          }
        },
        "68c4cf61318943cb88bc155dfe8c6b89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ab3d498e2e347f38c12e071fd0ca9d4",
            "max": 379,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2ddab6a6b0bc420f9c7599805a69e0d9",
            "value": 14
          }
        },
        "66be6ae60f464dd6b1fe82740b45ae0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db4cbd2f04ad403fb04a01a9de604d02",
            "placeholder": "​",
            "style": "IPY_MODEL_25e084e4cb0549bea3269298627ade20",
            "value": " 14/379 [00:04&lt;01:26,  4.24it/s, loss=0.00215]"
          }
        },
        "8c88a1a25d79443c83db625da7a7ecb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0130f6e35418428aad81460152ce3eb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78eed74360f841ed9a89db3afeaa8577": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ab3d498e2e347f38c12e071fd0ca9d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ddab6a6b0bc420f9c7599805a69e0d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db4cbd2f04ad403fb04a01a9de604d02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25e084e4cb0549bea3269298627ade20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95c123afc6f84bc7953bb96d0d56a6e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6759c49d4d5f4882b6829d2699da0034",
              "IPY_MODEL_82031f6f157a43859ab1694fe364c85b",
              "IPY_MODEL_c016aa08f05547c58cfca2dd74ea5672"
            ],
            "layout": "IPY_MODEL_f3ecb90af134446bb26e1850aaf8c6d3"
          }
        },
        "6759c49d4d5f4882b6829d2699da0034": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8e9ca44f80342889cd919d904ca3606",
            "placeholder": "​",
            "style": "IPY_MODEL_7b6afa435fa7467bbe0ee2a89426a489",
            "value": "Eval PushTImageEnv:  81%"
          }
        },
        "82031f6f157a43859ab1694fe364c85b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba900afe5b084ce18c23780a99346f53",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a17da852e4104ba681f356a8d2c454aa",
            "value": 162
          }
        },
        "c016aa08f05547c58cfca2dd74ea5672": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7cfe0c39c80488aba61edbe3e570653",
            "placeholder": "​",
            "style": "IPY_MODEL_104ecf3a8d0d441580f059ed101286b6",
            "value": " 162/200 [00:34&lt;00:06,  6.22it/s, reward=1]"
          }
        },
        "f3ecb90af134446bb26e1850aaf8c6d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8e9ca44f80342889cd919d904ca3606": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b6afa435fa7467bbe0ee2a89426a489": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba900afe5b084ce18c23780a99346f53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a17da852e4104ba681f356a8d2c454aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e7cfe0c39c80488aba61edbe3e570653": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "104ecf3a8d0d441580f059ed101286b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wrangelvid/diffusion-policy-dev/blob/add-notebooks/diffusion_policy_vision_pusht_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Installing pip packages**\n",
        "#@markdown - Diffusion Model: [PyTorch](https://pytorch.org) & [HuggingFace diffusers](https://huggingface.co/docs/diffusers/index)\n",
        "#@markdown - Dataset Loading: [Zarr](https://zarr.readthedocs.io/en/stable/) & numcodecs\n",
        "#@markdown - Push-T Env: gym, pygame, pymunk & shapely\n",
        "!python --version\n",
        "!pip3 uninstall cvxpy -y > /dev/null\n",
        "!pip3 install setuptools==65.5.0 > /dev/null\n",
        "# hack for gym==0.21.0 https://github.com/openai/gym/issues/3176\n",
        "!pip3 install torch==1.13.1 torchvision==0.14.1 diffusers==0.11.1 \\\n",
        "scikit-image==0.19.3 scikit-video==1.1.11 zarr==2.12.0 numcodecs==0.10.2 \\\n",
        "pygame==2.1.2 pymunk==6.2.1 gym==0.21.0 shapely==1.8.4 \\\n",
        "&> /dev/null # mute output"
      ],
      "metadata": {
        "id": "2QwO2gAgiJS2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66d448ef-325f-44ee-b1f0-294c6b1efac5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.9.16\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/wrangelvid/diffusion-policy-dev"
      ],
      "metadata": {
        "id": "9OgERoel91QN",
        "outputId": "9df86fdd-2d52-4dd7-d9f7-11275cfe3626",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'diffusion-policy-dev'...\n",
            "remote: Enumerating objects: 494, done.\u001b[K\n",
            "remote: Counting objects: 100% (494/494), done.\u001b[K\n",
            "remote: Compressing objects: 100% (321/321), done.\u001b[K\n",
            "remote: Total 494 (delta 170), reused 488 (delta 167), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (494/494), 12.58 MiB | 8.21 MiB/s, done.\n",
            "Resolving deltas: 100% (170/170), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/diffusion-policy-dev')"
      ],
      "metadata": {
        "id": "EIuq4adV-Ztb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusion_policy.env.pusht.pusht_env import PushTEnv"
      ],
      "metadata": {
        "id": "--YtWAzSAGtY",
        "outputId": "c189d23f-337c-4e37-f8a0-aa77500cde05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pygame 2.1.2 (SDL 2.0.16, Python 3.9.16)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Imports**\n",
        "# diffusion policy import\n",
        "from typing import Tuple, Sequence, Dict, Union, Optional, Callable\n",
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import collections\n",
        "import zarr\n",
        "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
        "from diffusers.training_utils import EMAModel\n",
        "from diffusers.optimization import get_scheduler\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# env import\n",
        "import gym\n",
        "from gym import spaces\n",
        "import pygame\n",
        "import pymunk\n",
        "import pymunk.pygame_util\n",
        "from pymunk.space_debug_draw_options import SpaceDebugColor\n",
        "from pymunk.vec2d import Vec2d\n",
        "import shapely.geometry as sg\n",
        "import cv2\n",
        "import skimage.transform as st\n",
        "from skvideo.io import vwrite\n",
        "from IPython.display import Video\n",
        "import gdown\n",
        "import os"
      ],
      "metadata": {
        "id": "VrX4VTl5pYNq",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60684127-70e0-46c6-ffd6-ab98036b52f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pygame 2.1.2 (SDL 2.0.16, Python 3.8.10)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Environment**\n",
        "#@markdown Defines a PyMunk-based Push-T environment `PushTEnv`.\n",
        "#@markdown And it's subclass `PushTImageEnv`.\n",
        "#@markdown \n",
        "#@markdown **Goal**: push the gray T-block into the green area.\n",
        "#@markdown\n",
        "#@markdown Adapted from [Implicit Behavior Cloning](https://implicitbc.github.io/)\n",
        "\n",
        "\n",
        "positive_y_is_up: bool = False\n",
        "\"\"\"Make increasing values of y point upwards.\n",
        "\n",
        "When True::\n",
        "\n",
        "    y\n",
        "    ^\n",
        "    |      . (3, 3)\n",
        "    |\n",
        "    |   . (2, 2)\n",
        "    |\n",
        "    +------ > x\n",
        "    \n",
        "When False::\n",
        "\n",
        "    +------ > x\n",
        "    |\n",
        "    |   . (2, 2)\n",
        "    |\n",
        "    |      . (3, 3)\n",
        "    v\n",
        "    y\n",
        "    \n",
        "\"\"\"\n",
        "\n",
        "def to_pygame(p: Tuple[float, float], surface: pygame.Surface) -> Tuple[int, int]:\n",
        "    \"\"\"Convenience method to convert pymunk coordinates to pygame surface\n",
        "    local coordinates.\n",
        "\n",
        "    Note that in case positive_y_is_up is False, this function wont actually do\n",
        "    anything except converting the point to integers.\n",
        "    \"\"\"\n",
        "    if positive_y_is_up:\n",
        "        return round(p[0]), surface.get_height() - round(p[1])\n",
        "    else:\n",
        "        return round(p[0]), round(p[1])\n",
        "\n",
        "\n",
        "def light_color(color: SpaceDebugColor):\n",
        "    color = np.minimum(1.2 * np.float32([color.r, color.g, color.b, color.a]), np.float32([255]))\n",
        "    color = SpaceDebugColor(r=color[0], g=color[1], b=color[2], a=color[3])\n",
        "    return color\n",
        "\n",
        "class DrawOptions(pymunk.SpaceDebugDrawOptions):\n",
        "    def __init__(self, surface: pygame.Surface) -> None:\n",
        "        \"\"\"Draw a pymunk.Space on a pygame.Surface object.\n",
        "\n",
        "        Typical usage::\n",
        "\n",
        "        >>> import pymunk\n",
        "        >>> surface = pygame.Surface((10,10))\n",
        "        >>> space = pymunk.Space()\n",
        "        >>> options = pymunk.pygame_util.DrawOptions(surface)\n",
        "        >>> space.debug_draw(options)\n",
        "\n",
        "        You can control the color of a shape by setting shape.color to the color\n",
        "        you want it drawn in::\n",
        "\n",
        "        >>> c = pymunk.Circle(None, 10)\n",
        "        >>> c.color = pygame.Color(\"pink\")\n",
        "\n",
        "        See pygame_util.demo.py for a full example\n",
        "\n",
        "        Since pygame uses a coordiante system where y points down (in contrast\n",
        "        to many other cases), you either have to make the physics simulation\n",
        "        with Pymunk also behave in that way, or flip everything when you draw.\n",
        "\n",
        "        The easiest is probably to just make the simulation behave the same\n",
        "        way as Pygame does. In that way all coordinates used are in the same\n",
        "        orientation and easy to reason about::\n",
        "\n",
        "        >>> space = pymunk.Space()\n",
        "        >>> space.gravity = (0, -1000)\n",
        "        >>> body = pymunk.Body()\n",
        "        >>> body.position = (0, 0) # will be positioned in the top left corner\n",
        "        >>> space.debug_draw(options)\n",
        "\n",
        "        To flip the drawing its possible to set the module property\n",
        "        :py:data:`positive_y_is_up` to True. Then the pygame drawing will flip\n",
        "        the simulation upside down before drawing::\n",
        "\n",
        "        >>> positive_y_is_up = True\n",
        "        >>> body = pymunk.Body()\n",
        "        >>> body.position = (0, 0)\n",
        "        >>> # Body will be position in bottom left corner\n",
        "\n",
        "        :Parameters:\n",
        "                surface : pygame.Surface\n",
        "                    Surface that the objects will be drawn on\n",
        "        \"\"\"\n",
        "        self.surface = surface\n",
        "        super(DrawOptions, self).__init__()\n",
        "\n",
        "    def draw_circle(\n",
        "        self,\n",
        "        pos: Vec2d,\n",
        "        angle: float,\n",
        "        radius: float,\n",
        "        outline_color: SpaceDebugColor,\n",
        "        fill_color: SpaceDebugColor,\n",
        "    ) -> None:\n",
        "        p = to_pygame(pos, self.surface)\n",
        "\n",
        "        pygame.draw.circle(self.surface, fill_color.as_int(), p, round(radius), 0)\n",
        "        pygame.draw.circle(self.surface, light_color(fill_color).as_int(), p, round(radius-4), 0)\n",
        "\n",
        "        circle_edge = pos + Vec2d(radius, 0).rotated(angle)\n",
        "        p2 = to_pygame(circle_edge, self.surface)\n",
        "        line_r = 2 if radius > 20 else 1\n",
        "        # pygame.draw.lines(self.surface, outline_color.as_int(), False, [p, p2], line_r)\n",
        "\n",
        "    def draw_segment(self, a: Vec2d, b: Vec2d, color: SpaceDebugColor) -> None:\n",
        "        p1 = to_pygame(a, self.surface)\n",
        "        p2 = to_pygame(b, self.surface)\n",
        "\n",
        "        pygame.draw.aalines(self.surface, color.as_int(), False, [p1, p2])\n",
        "\n",
        "    def draw_fat_segment(\n",
        "        self,\n",
        "        a: Tuple[float, float],\n",
        "        b: Tuple[float, float],\n",
        "        radius: float,\n",
        "        outline_color: SpaceDebugColor,\n",
        "        fill_color: SpaceDebugColor,\n",
        "    ) -> None:\n",
        "        p1 = to_pygame(a, self.surface)\n",
        "        p2 = to_pygame(b, self.surface)\n",
        "\n",
        "        r = round(max(1, radius * 2))\n",
        "        pygame.draw.lines(self.surface, fill_color.as_int(), False, [p1, p2], r)\n",
        "        if r > 2:\n",
        "            orthog = [abs(p2[1] - p1[1]), abs(p2[0] - p1[0])]\n",
        "            if orthog[0] == 0 and orthog[1] == 0:\n",
        "                return\n",
        "            scale = radius / (orthog[0] * orthog[0] + orthog[1] * orthog[1]) ** 0.5\n",
        "            orthog[0] = round(orthog[0] * scale)\n",
        "            orthog[1] = round(orthog[1] * scale)\n",
        "            points = [\n",
        "                (p1[0] - orthog[0], p1[1] - orthog[1]),\n",
        "                (p1[0] + orthog[0], p1[1] + orthog[1]),\n",
        "                (p2[0] + orthog[0], p2[1] + orthog[1]),\n",
        "                (p2[0] - orthog[0], p2[1] - orthog[1]),\n",
        "            ]\n",
        "            pygame.draw.polygon(self.surface, fill_color.as_int(), points)\n",
        "            pygame.draw.circle(\n",
        "                self.surface,\n",
        "                fill_color.as_int(),\n",
        "                (round(p1[0]), round(p1[1])),\n",
        "                round(radius),\n",
        "            )\n",
        "            pygame.draw.circle(\n",
        "                self.surface,\n",
        "                fill_color.as_int(),\n",
        "                (round(p2[0]), round(p2[1])),\n",
        "                round(radius),\n",
        "            )\n",
        "\n",
        "    def draw_polygon(\n",
        "        self,\n",
        "        verts: Sequence[Tuple[float, float]],\n",
        "        radius: float,\n",
        "        outline_color: SpaceDebugColor,\n",
        "        fill_color: SpaceDebugColor,\n",
        "    ) -> None:\n",
        "        ps = [to_pygame(v, self.surface) for v in verts]\n",
        "        ps += [ps[0]]\n",
        "\n",
        "        radius = 2\n",
        "        pygame.draw.polygon(self.surface, light_color(fill_color).as_int(), ps)\n",
        "\n",
        "        if radius > 0:\n",
        "            for i in range(len(verts)):\n",
        "                a = verts[i]\n",
        "                b = verts[(i + 1) % len(verts)]\n",
        "                self.draw_fat_segment(a, b, radius, fill_color, fill_color)\n",
        "\n",
        "    def draw_dot(\n",
        "        self, size: float, pos: Tuple[float, float], color: SpaceDebugColor\n",
        "    ) -> None:\n",
        "        p = to_pygame(pos, self.surface)\n",
        "        pygame.draw.circle(self.surface, color.as_int(), p, round(size), 0)\n",
        "\n",
        "\n",
        "def pymunk_to_shapely(body, shapes):\n",
        "    geoms = list()\n",
        "    for shape in shapes:\n",
        "        if isinstance(shape, pymunk.shapes.Poly):\n",
        "            verts = [body.local_to_world(v) for v in shape.get_vertices()]\n",
        "            verts += [verts[0]]\n",
        "            geoms.append(sg.Polygon(verts))\n",
        "        else:\n",
        "            raise RuntimeError(f'Unsupported shape type {type(shape)}')\n",
        "    geom = sg.MultiPolygon(geoms)\n",
        "    return geom\n",
        "\n",
        "# env\n",
        "class PushTEnv(gym.Env):\n",
        "    metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": 10}\n",
        "    reward_range = (0., 1.)\n",
        "\n",
        "    def __init__(self,\n",
        "            legacy=False, \n",
        "            block_cog=None, damping=None,\n",
        "            render_action=True,\n",
        "            render_size=96,\n",
        "            reset_to_state=None\n",
        "        ):\n",
        "        self._seed = None\n",
        "        self.seed()\n",
        "        self.window_size = ws = 512  # The size of the PyGame window\n",
        "        self.render_size = render_size\n",
        "        self.sim_hz = 100\n",
        "        # Local controller params.\n",
        "        self.k_p, self.k_v = 100, 20    # PD control.z\n",
        "        self.control_hz = self.metadata['video.frames_per_second']\n",
        "        # legcay set_state for data compatiblity\n",
        "        self.legacy = legacy\n",
        "\n",
        "        # agent_pos, block_pos, block_angle\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.array([0,0,0,0,0], dtype=np.float64),\n",
        "            high=np.array([ws,ws,ws,ws,np.pi*2], dtype=np.float64),\n",
        "            shape=(5,),\n",
        "            dtype=np.float64\n",
        "        )\n",
        "\n",
        "        # positional goal for agent\n",
        "        self.action_space = spaces.Box(\n",
        "            low=np.array([0,0], dtype=np.float64),\n",
        "            high=np.array([ws,ws], dtype=np.float64),\n",
        "            shape=(2,),\n",
        "            dtype=np.float64\n",
        "        )\n",
        "\n",
        "        self.block_cog = block_cog\n",
        "        self.damping = damping\n",
        "        self.render_action = render_action\n",
        "\n",
        "        \"\"\"\n",
        "        If human-rendering is used, `self.window` will be a reference\n",
        "        to the window that we draw to. `self.clock` will be a clock that is used\n",
        "        to ensure that the environment is rendered at the correct framerate in\n",
        "        human-mode. They will remain `None` until human-mode is used for the\n",
        "        first time.\n",
        "        \"\"\"\n",
        "        self.window = None\n",
        "        self.clock = None\n",
        "        self.screen = None\n",
        "\n",
        "        self.space = None\n",
        "        self.teleop = None\n",
        "        self.render_buffer = None\n",
        "        self.latest_action = None\n",
        "        self.reset_to_state = reset_to_state\n",
        "    \n",
        "    def reset(self):\n",
        "        seed = self._seed\n",
        "        self._setup()\n",
        "        if self.block_cog is not None:\n",
        "            self.block.center_of_gravity = self.block_cog\n",
        "        if self.damping is not None:\n",
        "            self.space.damping = self.damping\n",
        "        \n",
        "        # use legacy RandomState for compatiblity\n",
        "        state = self.reset_to_state\n",
        "        if state is None:\n",
        "            rs = np.random.RandomState(seed=seed)\n",
        "            state = np.array([\n",
        "                rs.randint(50, 450), rs.randint(50, 450),\n",
        "                rs.randint(100, 400), rs.randint(100, 400),\n",
        "                rs.randn() * 2 * np.pi - np.pi\n",
        "                ])\n",
        "        self._set_state(state)\n",
        "\n",
        "        observation = self._get_obs()\n",
        "        return observation\n",
        "\n",
        "    def step(self, action):\n",
        "        dt = 1.0 / self.sim_hz\n",
        "        self.n_contact_points = 0\n",
        "        n_steps = self.sim_hz // self.control_hz\n",
        "        if action is not None:\n",
        "            self.latest_action = action\n",
        "            for i in range(n_steps):\n",
        "                # Step PD control.\n",
        "                # self.agent.velocity = self.k_p * (act - self.agent.position)    # P control works too.\n",
        "                acceleration = self.k_p * (action - self.agent.position) + self.k_v * (Vec2d(0, 0) - self.agent.velocity)\n",
        "                self.agent.velocity += acceleration * dt\n",
        "\n",
        "                # Step physics.\n",
        "                self.space.step(dt)\n",
        "\n",
        "        # compute reward\n",
        "        goal_body = self._get_goal_pose_body(self.goal_pose)\n",
        "        goal_geom = pymunk_to_shapely(goal_body, self.block.shapes)\n",
        "        block_geom = pymunk_to_shapely(self.block, self.block.shapes)\n",
        "\n",
        "        intersection_area = goal_geom.intersection(block_geom).area\n",
        "        goal_area = goal_geom.area\n",
        "        coverage = intersection_area / goal_area\n",
        "        reward = np.clip(coverage / self.success_threshold, 0, 1)\n",
        "        done = coverage > self.success_threshold\n",
        "\n",
        "        observation = self._get_obs()\n",
        "        info = self._get_info()\n",
        "\n",
        "        return observation, reward, done, info\n",
        "\n",
        "    def render(self, mode):\n",
        "        return self._render_frame(mode)\n",
        "\n",
        "    def teleop_agent(self):\n",
        "        TeleopAgent = collections.namedtuple('TeleopAgent', ['act'])\n",
        "        def act(obs):\n",
        "            act = None\n",
        "            mouse_position = pymunk.pygame_util.from_pygame(Vec2d(*pygame.mouse.get_pos()), self.screen)\n",
        "            if self.teleop or (mouse_position - self.agent.position).length < 30:\n",
        "                self.teleop = True\n",
        "                act = mouse_position\n",
        "            return act\n",
        "        return TeleopAgent(act)\n",
        "\n",
        "    def _get_obs(self):\n",
        "        obs = np.array(\n",
        "            tuple(self.agent.position) \\\n",
        "            + tuple(self.block.position) \\\n",
        "            + (self.block.angle % (2 * np.pi),))\n",
        "        return obs\n",
        "\n",
        "    def _get_goal_pose_body(self, pose):\n",
        "        mass = 1\n",
        "        inertia = pymunk.moment_for_box(mass, (50, 100))\n",
        "        body = pymunk.Body(mass, inertia)\n",
        "        # preserving the legacy assignment order for compatibility\n",
        "        # the order here dosn't matter somehow, maybe because CoM is aligned with body origin\n",
        "        body.position = pose[:2].tolist()\n",
        "        body.angle = pose[2]\n",
        "        return body\n",
        "    \n",
        "    def _get_info(self):\n",
        "        n_steps = self.sim_hz // self.control_hz\n",
        "        n_contact_points_per_step = int(np.ceil(self.n_contact_points / n_steps))\n",
        "        info = {\n",
        "            'pos_agent': np.array(self.agent.position),\n",
        "            'vel_agent': np.array(self.agent.velocity),\n",
        "            'block_pose': np.array(list(self.block.position) + [self.block.angle]),\n",
        "            'goal_pose': self.goal_pose,\n",
        "            'n_contacts': n_contact_points_per_step}\n",
        "        return info\n",
        "\n",
        "    def _render_frame(self, mode):\n",
        "\n",
        "        if self.window is None and mode == \"human\":\n",
        "            pygame.init()\n",
        "            pygame.display.init()\n",
        "            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
        "        if self.clock is None and mode == \"human\":\n",
        "            self.clock = pygame.time.Clock()\n",
        "\n",
        "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
        "        canvas.fill((255, 255, 255))\n",
        "        self.screen = canvas\n",
        "\n",
        "        draw_options = DrawOptions(canvas)\n",
        "\n",
        "        # Draw goal pose.\n",
        "        goal_body = self._get_goal_pose_body(self.goal_pose)\n",
        "        for shape in self.block.shapes:\n",
        "            goal_points = [pymunk.pygame_util.to_pygame(goal_body.local_to_world(v), draw_options.surface) for v in shape.get_vertices()]\n",
        "            goal_points += [goal_points[0]]\n",
        "            pygame.draw.polygon(canvas, self.goal_color, goal_points)\n",
        "\n",
        "        # Draw agent and block.\n",
        "        self.space.debug_draw(draw_options)\n",
        "\n",
        "        if mode == \"human\":\n",
        "            # The following line copies our drawings from `canvas` to the visible window\n",
        "            self.window.blit(canvas, canvas.get_rect())\n",
        "            pygame.event.pump()\n",
        "            pygame.display.update()\n",
        "\n",
        "            # the clock is aleady ticked during in step for \"human\"\n",
        "\n",
        "\n",
        "        img = np.transpose(\n",
        "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
        "            )\n",
        "        img = cv2.resize(img, (self.render_size, self.render_size))\n",
        "        if self.render_action:\n",
        "            if self.render_action and (self.latest_action is not None):\n",
        "                action = np.array(self.latest_action)\n",
        "                coord = (action / 512 * 96).astype(np.int32)\n",
        "                marker_size = int(8/96*self.render_size)\n",
        "                thickness = int(1/96*self.render_size)\n",
        "                cv2.drawMarker(img, coord,\n",
        "                    color=(255,0,0), markerType=cv2.MARKER_CROSS,\n",
        "                    markerSize=marker_size, thickness=thickness)\n",
        "        return img\n",
        "\n",
        "\n",
        "    def close(self):\n",
        "        if self.window is not None:\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()\n",
        "    \n",
        "    def seed(self, seed=None):\n",
        "        if seed is None:\n",
        "            seed = np.random.randint(0,25536)\n",
        "        self._seed = seed\n",
        "        self.np_random = np.random.default_rng(seed)\n",
        "\n",
        "    def _handle_collision(self, arbiter, space, data):\n",
        "        self.n_contact_points += len(arbiter.contact_point_set.points)\n",
        "\n",
        "    def _set_state(self, state):\n",
        "        if isinstance(state, np.ndarray):\n",
        "            state = state.tolist()\n",
        "        pos_agent = state[:2]\n",
        "        pos_block = state[2:4]\n",
        "        rot_block = state[4]\n",
        "        self.agent.position = pos_agent\n",
        "        # setting angle rotates with respect to center of mass\n",
        "        # therefore will modify the geometric position\n",
        "        # if not the same as CoM\n",
        "        # therefore should be modified first.\n",
        "        if self.legacy:\n",
        "            # for compatiblity with legacy data\n",
        "            self.block.position = pos_block\n",
        "            self.block.angle = rot_block\n",
        "        else:\n",
        "            self.block.angle = rot_block\n",
        "            self.block.position = pos_block\n",
        "\n",
        "        # Run physics to take effect\n",
        "        self.space.step(1.0 / self.sim_hz)\n",
        "    \n",
        "    def _set_state_local(self, state_local):\n",
        "        agent_pos_local = state_local[:2]\n",
        "        block_pose_local = state_local[2:]\n",
        "        tf_img_obj = st.AffineTransform(\n",
        "            translation=self.goal_pose[:2], \n",
        "            rotation=self.goal_pose[2])\n",
        "        tf_obj_new = st.AffineTransform(\n",
        "            translation=block_pose_local[:2],\n",
        "            rotation=block_pose_local[2]\n",
        "        )\n",
        "        tf_img_new = st.AffineTransform(\n",
        "            matrix=tf_img_obj.params @ tf_obj_new.params\n",
        "        )\n",
        "        agent_pos_new = tf_img_new(agent_pos_local)\n",
        "        new_state = np.array(\n",
        "            list(agent_pos_new[0]) + list(tf_img_new.translation) \\\n",
        "                + [tf_img_new.rotation])\n",
        "        self._set_state(new_state)\n",
        "        return new_state\n",
        "\n",
        "    def _setup(self):\n",
        "        self.space = pymunk.Space()\n",
        "        self.space.gravity = 0, 0\n",
        "        self.space.damping = 0\n",
        "        self.teleop = False\n",
        "        self.render_buffer = list()\n",
        "        \n",
        "        # Add walls.\n",
        "        walls = [\n",
        "            self._add_segment((5, 506), (5, 5), 2),\n",
        "            self._add_segment((5, 5), (506, 5), 2),\n",
        "            self._add_segment((506, 5), (506, 506), 2),\n",
        "            self._add_segment((5, 506), (506, 506), 2)\n",
        "        ]\n",
        "        self.space.add(*walls)\n",
        "\n",
        "        # Add agent, block, and goal zone.\n",
        "        self.agent = self.add_circle((256, 400), 15)\n",
        "        self.block = self.add_tee((256, 300), 0)\n",
        "        self.goal_color = pygame.Color('LightGreen')\n",
        "        self.goal_pose = np.array([256,256,np.pi/4])  # x, y, theta (in radians)\n",
        "\n",
        "        # Add collision handeling\n",
        "        self.collision_handeler = self.space.add_collision_handler(0, 0)\n",
        "        self.collision_handeler.post_solve = self._handle_collision\n",
        "        self.n_contact_points = 0\n",
        "\n",
        "        self.max_score = 50 * 100\n",
        "        self.success_threshold = 0.95    # 95% coverage.\n",
        "\n",
        "    def _add_segment(self, a, b, radius):\n",
        "        shape = pymunk.Segment(self.space.static_body, a, b, radius)\n",
        "        shape.color = pygame.Color('LightGray')    # https://htmlcolorcodes.com/color-names\n",
        "        return shape\n",
        "\n",
        "    def add_circle(self, position, radius):\n",
        "        body = pymunk.Body(body_type=pymunk.Body.KINEMATIC)\n",
        "        body.position = position\n",
        "        body.friction = 1\n",
        "        shape = pymunk.Circle(body, radius)\n",
        "        shape.color = pygame.Color('RoyalBlue')\n",
        "        self.space.add(body, shape)\n",
        "        return body\n",
        "\n",
        "    def add_box(self, position, height, width):\n",
        "        mass = 1\n",
        "        inertia = pymunk.moment_for_box(mass, (height, width))\n",
        "        body = pymunk.Body(mass, inertia)\n",
        "        body.position = position\n",
        "        shape = pymunk.Poly.create_box(body, (height, width))\n",
        "        shape.color = pygame.Color('LightSlateGray')\n",
        "        self.space.add(body, shape)\n",
        "        return body\n",
        "\n",
        "    def add_tee(self, position, angle, scale=30, color='LightSlateGray', mask=pymunk.ShapeFilter.ALL_MASKS()):\n",
        "        mass = 1\n",
        "        length = 4\n",
        "        vertices1 = [(-length*scale/2, scale),\n",
        "                                 ( length*scale/2, scale),\n",
        "                                 ( length*scale/2, 0),\n",
        "                                 (-length*scale/2, 0)]\n",
        "        inertia1 = pymunk.moment_for_poly(mass, vertices=vertices1)\n",
        "        vertices2 = [(-scale/2, scale),\n",
        "                                 (-scale/2, length*scale),\n",
        "                                 ( scale/2, length*scale),\n",
        "                                 ( scale/2, scale)]\n",
        "        inertia2 = pymunk.moment_for_poly(mass, vertices=vertices1)\n",
        "        body = pymunk.Body(mass, inertia1 + inertia2)\n",
        "        shape1 = pymunk.Poly(body, vertices1)\n",
        "        shape2 = pymunk.Poly(body, vertices2)\n",
        "        shape1.color = pygame.Color(color)\n",
        "        shape2.color = pygame.Color(color)\n",
        "        shape1.filter = pymunk.ShapeFilter(mask=mask)\n",
        "        shape2.filter = pymunk.ShapeFilter(mask=mask)\n",
        "        body.center_of_gravity = (shape1.center_of_gravity + shape2.center_of_gravity) / 2\n",
        "        body.position = position\n",
        "        body.angle = angle\n",
        "        body.friction = 1\n",
        "        self.space.add(body, shape1, shape2)\n",
        "        return body\n",
        "\n",
        "\n",
        "class PushTImageEnv(PushTEnv):\n",
        "    metadata = {\"render.modes\": [\"rgb_array\"], \"video.frames_per_second\": 10}\n",
        "\n",
        "    def __init__(self,\n",
        "            legacy=False,\n",
        "            block_cog=None, \n",
        "            damping=None,\n",
        "            render_size=96):\n",
        "        super().__init__(\n",
        "            legacy=legacy, \n",
        "            block_cog=block_cog,\n",
        "            damping=damping,\n",
        "            render_size=render_size,\n",
        "            render_action=False)\n",
        "        ws = self.window_size\n",
        "        self.observation_space = spaces.Dict({\n",
        "            'image': spaces.Box(\n",
        "                low=0,\n",
        "                high=1,\n",
        "                shape=(3,render_size,render_size),\n",
        "                dtype=np.float32\n",
        "            ),\n",
        "            'agent_pos': spaces.Box(\n",
        "                low=0,\n",
        "                high=ws,\n",
        "                shape=(2,),\n",
        "                dtype=np.float32\n",
        "            )\n",
        "        })\n",
        "        self.render_cache = None\n",
        "    \n",
        "    def _get_obs(self):\n",
        "        img = super()._render_frame(mode='rgb_array')\n",
        "\n",
        "        agent_pos = np.array(self.agent.position)\n",
        "        img_obs = np.moveaxis(img.astype(np.float32) / 255, -1, 0)\n",
        "        obs = {\n",
        "            'image': img_obs,\n",
        "            'agent_pos': agent_pos\n",
        "        }\n",
        "\n",
        "        # draw action\n",
        "        if self.latest_action is not None:\n",
        "            action = np.array(self.latest_action)\n",
        "            coord = (action / 512 * 96).astype(np.int32)\n",
        "            marker_size = int(8/96*self.render_size)\n",
        "            thickness = int(1/96*self.render_size)\n",
        "            cv2.drawMarker(img, coord,\n",
        "                color=(255,0,0), markerType=cv2.MARKER_CROSS,\n",
        "                markerSize=marker_size, thickness=thickness)\n",
        "        self.render_cache = img\n",
        "\n",
        "        return obs\n",
        "\n",
        "    def render(self, mode):\n",
        "        assert mode == 'rgb_array'\n",
        "\n",
        "        if self.render_cache is None:\n",
        "            self._get_obs()\n",
        "        \n",
        "        return self.render_cache\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "L5E-nR6ornyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Env Demo**\n",
        "#@markdown Standard Gym Env (0.21.0 API)\n",
        "\n",
        "# 0. create env object\n",
        "env = PushTImageEnv()\n",
        "\n",
        "# 1. seed env for initial state. \n",
        "# Seed 0-200 are used for the demonstration dataset.\n",
        "env.seed(1000)\n",
        "\n",
        "# 2. must reset before use\n",
        "obs = env.reset()\n",
        "\n",
        "# 3. 2D positional action space [0,512]\n",
        "action = env.action_space.sample()\n",
        "\n",
        "# 4. Standard gym step method\n",
        "obs, reward, done, info = env.step(action)\n",
        "\n",
        "# prints and explains each dimension of the observation and action vectors\n",
        "with np.printoptions(precision=4, suppress=True, threshold=5):\n",
        "    print(\"obs['image'].shape:\", obs['image'].shape, \"float32, [0,1]\")\n",
        "    print(\"obs['agent_pos'].shape:\", obs['agent_pos'].shape, \"float32, [0,512]\")\n",
        "    print(\"action.shape: \", action.shape, \"float32, [0,512]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OknH8Qfqrtc9",
        "outputId": "fac36b6c-a915-47e6-f3cc-56a350db1eb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "obs['image'].shape: (3, 96, 96) float32, [0,1]\n",
            "obs['agent_pos'].shape: (2,) float32, [0,512]\n",
            "action.shape:  (2,) float32, [0,512]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Dataset**\n",
        "#@markdown\n",
        "#@markdown Defines `PushTImageDataset` and helper functions\n",
        "#@markdown\n",
        "#@markdown The dataset class\n",
        "#@markdown - Load data ((image, agent_pos), action) from a zarr storage\n",
        "#@markdown - Normalizes each dimension of agent_pos and action to [-1,1]\n",
        "#@markdown - Returns\n",
        "#@markdown  - All possible segments with length `pred_horizon`\n",
        "#@markdown  - Pads the beginning and the end of each episode with repetition\n",
        "#@markdown  - key `image`: shape (obs_hoirzon, 3, 96, 96)\n",
        "#@markdown  - key `agent_pos`: shape (obs_hoirzon, 2)\n",
        "#@markdown  - key `action`: shape (pred_horizon, 2) \n",
        "\n",
        "def create_sample_indices(\n",
        "        episode_ends:np.ndarray, sequence_length:int, \n",
        "        pad_before: int=0, pad_after: int=0):\n",
        "    indices = list()\n",
        "    for i in range(len(episode_ends)):\n",
        "        start_idx = 0\n",
        "        if i > 0:\n",
        "            start_idx = episode_ends[i-1]\n",
        "        end_idx = episode_ends[i]\n",
        "        episode_length = end_idx - start_idx\n",
        "        \n",
        "        min_start = -pad_before\n",
        "        max_start = episode_length - sequence_length + pad_after\n",
        "        \n",
        "        # range stops one idx before end\n",
        "        for idx in range(min_start, max_start+1):\n",
        "            buffer_start_idx = max(idx, 0) + start_idx\n",
        "            buffer_end_idx = min(idx+sequence_length, episode_length) + start_idx\n",
        "            start_offset = buffer_start_idx - (idx+start_idx)\n",
        "            end_offset = (idx+sequence_length+start_idx) - buffer_end_idx\n",
        "            sample_start_idx = 0 + start_offset\n",
        "            sample_end_idx = sequence_length - end_offset\n",
        "            indices.append([\n",
        "                buffer_start_idx, buffer_end_idx, \n",
        "                sample_start_idx, sample_end_idx])\n",
        "    indices = np.array(indices)\n",
        "    return indices\n",
        "\n",
        "\n",
        "def sample_sequence(train_data, sequence_length,\n",
        "                    buffer_start_idx, buffer_end_idx, \n",
        "                    sample_start_idx, sample_end_idx):\n",
        "    result = dict()\n",
        "    for key, input_arr in train_data.items():\n",
        "        sample = input_arr[buffer_start_idx:buffer_end_idx]\n",
        "        data = sample\n",
        "        if (sample_start_idx > 0) or (sample_end_idx < sequence_length):\n",
        "            data = np.zeros(\n",
        "                shape=(sequence_length,) + input_arr.shape[1:],\n",
        "                dtype=input_arr.dtype)\n",
        "            if sample_start_idx > 0:\n",
        "                data[:sample_start_idx] = sample[0]\n",
        "            if sample_end_idx < sequence_length:\n",
        "                data[sample_end_idx:] = sample[-1]\n",
        "            data[sample_start_idx:sample_end_idx] = sample\n",
        "        result[key] = data\n",
        "    return result\n",
        "\n",
        "# normalize data\n",
        "def get_data_stats(data):\n",
        "    data = data.reshape(-1,data.shape[-1])\n",
        "    stats = {\n",
        "        'min': np.min(data, axis=0),\n",
        "        'max': np.max(data, axis=0)\n",
        "    }\n",
        "    return stats\n",
        "\n",
        "def normalize_data(data, stats):\n",
        "    # nomalize to [0,1]\n",
        "    ndata = (data - stats['min']) / (stats['max'] - stats['min'])\n",
        "    # normalize to [-1, 1]\n",
        "    ndata = ndata * 2 - 1\n",
        "    return ndata\n",
        "\n",
        "def unnormalize_data(ndata, stats):\n",
        "    ndata = (ndata + 1) / 2\n",
        "    data = ndata * (stats['max'] - stats['min']) + stats['min']\n",
        "    return data\n",
        "\n",
        "# dataset\n",
        "class PushTImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, \n",
        "                 dataset_path: str,\n",
        "                 pred_horizon: int, \n",
        "                 obs_horizon: int, \n",
        "                 action_horizon: int):\n",
        "        \n",
        "        # read from zarr dataset\n",
        "        dataset_root = zarr.open(dataset_path, 'r')\n",
        "        \n",
        "        # float32, [0,1], (N,96,96,3)\n",
        "        train_image_data = dataset_root['data']['img'][:]\n",
        "        train_image_data = np.moveaxis(train_image_data, -1,1)\n",
        "        # (N,3,96,96)\n",
        "\n",
        "        # (N, D)\n",
        "        train_data = {\n",
        "            # first two dims of state vector are agent (i.e. gripper) locations\n",
        "            'agent_pos': dataset_root['data']['state'][:,:2],\n",
        "            'action': dataset_root['data']['action'][:]\n",
        "        }\n",
        "        episode_ends = dataset_root['meta']['episode_ends'][:]\n",
        "        \n",
        "        # compute start and end of each state-action sequence\n",
        "        # also handles padding\n",
        "        indices = create_sample_indices(\n",
        "            episode_ends=episode_ends,\n",
        "            sequence_length=pred_horizon,\n",
        "            pad_before=obs_horizon-1,\n",
        "            pad_after=action_horizon-1)\n",
        "\n",
        "        # compute statistics and normalized data to [-1,1]\n",
        "        stats = dict()\n",
        "        normalized_train_data = dict()\n",
        "        for key, data in train_data.items():\n",
        "            stats[key] = get_data_stats(data)\n",
        "            normalized_train_data[key] = normalize_data(data, stats[key])\n",
        "        \n",
        "        # images are already normalized\n",
        "        normalized_train_data['image'] = train_image_data\n",
        "\n",
        "        self.indices = indices\n",
        "        self.stats = stats\n",
        "        self.normalized_train_data = normalized_train_data\n",
        "        self.pred_horizon = pred_horizon\n",
        "        self.action_horizon = action_horizon\n",
        "        self.obs_horizon = obs_horizon\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # get the start/end indices for this datapoint\n",
        "        buffer_start_idx, buffer_end_idx, \\\n",
        "            sample_start_idx, sample_end_idx = self.indices[idx]\n",
        "\n",
        "        # get nomralized data using these indices\n",
        "        nsample = sample_sequence(\n",
        "            train_data=self.normalized_train_data,\n",
        "            sequence_length=self.pred_horizon,\n",
        "            buffer_start_idx=buffer_start_idx,\n",
        "            buffer_end_idx=buffer_end_idx,\n",
        "            sample_start_idx=sample_start_idx,\n",
        "            sample_end_idx=sample_end_idx\n",
        "        )\n",
        "\n",
        "        # discard unused observations\n",
        "        nsample['image'] = nsample['image'][:self.obs_horizon,:]\n",
        "        nsample['agent_pos'] = nsample['agent_pos'][:self.obs_horizon,:]\n",
        "        return nsample\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vHepJOFBucwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Dataset Demo**\n",
        "\n",
        "# download demonstration data from Google Drive\n",
        "dataset_path = \"pusht_cchi_v7_replay.zarr.zip\"\n",
        "if not os.path.isfile(dataset_path):\n",
        "    id = \"1KY1InLurpMvJDRb14L9NlXT_fEsCvVUq&confirm=t\"\n",
        "    gdown.download(id=id, output=dataset_path, quiet=False)\n",
        "\n",
        "# parameters\n",
        "pred_horizon = 16\n",
        "obs_horizon = 2\n",
        "action_horizon = 8\n",
        "#|o|o|                             observations: 2\n",
        "#| |a|a|a|a|a|a|a|a|               actions executed: 8\n",
        "#|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p| actions predicted: 16\n",
        "\n",
        "# create dataset from file\n",
        "dataset = PushTImageDataset(\n",
        "    dataset_path=dataset_path,\n",
        "    pred_horizon=pred_horizon,\n",
        "    obs_horizon=obs_horizon,\n",
        "    action_horizon=action_horizon\n",
        ")\n",
        "# save training data statistics (min, max) for each dim\n",
        "stats = dataset.stats\n",
        "\n",
        "# create dataloader\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=64,\n",
        "    num_workers=4,\n",
        "    shuffle=True,\n",
        "    # accelerate cpu-gpu transfer\n",
        "    pin_memory=True, \n",
        "    # don't kill worker process afte each epoch\n",
        "    persistent_workers=True \n",
        ")\n",
        "\n",
        "# visualize data in batch\n",
        "batch = next(iter(dataloader))\n",
        "print(\"batch['image'].shape:\", batch['image'].shape)\n",
        "print(\"batch['agent_pos'].shape:\", batch['agent_pos'].shape)\n",
        "print(\"batch['action'].shape\", batch['action'].shape)"
      ],
      "metadata": {
        "id": "9ZiHF3lzvB6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f46cccc-6673-450c-d06b-e8c7e2e4e5ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1KY1InLurpMvJDRb14L9NlXT_fEsCvVUq&confirm=t\n",
            "To: /content/pusht_cchi_v7_replay.zarr.zip\n",
            "100%|██████████| 31.1M/31.1M [00:00<00:00, 103MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch['image'].shape: torch.Size([64, 2, 3, 96, 96])\n",
            "batch['agent_pos'].shape: torch.Size([64, 2, 2])\n",
            "batch['action'].shape torch.Size([64, 16, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Network**\n",
        "#@markdown\n",
        "#@markdown Defines a 1D UNet architecture `ConditionalUnet1D`\n",
        "#@markdown as the noies prediction network\n",
        "#@markdown\n",
        "#@markdown Components\n",
        "#@markdown - `SinusoidalPosEmb` Positional encoding for the diffusion iteration k\n",
        "#@markdown - `Downsample1d` Strided convolution to reduce temporal resolution\n",
        "#@markdown - `Upsample1d` Transposed convolution to increase temporal resolution\n",
        "#@markdown - `Conv1dBlock` Conv1d --> GroupNorm --> Mish\n",
        "#@markdown - `ConditionalResidualBlock1D` Takes two inputs `x` and `cond`. \\\n",
        "#@markdown `x` is passed through 2 `Conv1dBlock` stacked together with residual connection. \n",
        "#@markdown `cond` is applied to `x` with [FiLM](https://arxiv.org/abs/1709.07871) conditioning.\n",
        "\n",
        "class SinusoidalPosEmb(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        device = x.device\n",
        "        half_dim = self.dim // 2\n",
        "        emb = math.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
        "        emb = x[:, None] * emb[None, :]\n",
        "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
        "        return emb\n",
        "\n",
        "\n",
        "class Downsample1d(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv1d(dim, dim, 3, 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class Upsample1d(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.conv = nn.ConvTranspose1d(dim, dim, 4, 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class Conv1dBlock(nn.Module):\n",
        "    '''\n",
        "        Conv1d --> GroupNorm --> Mish\n",
        "    '''\n",
        "\n",
        "    def __init__(self, inp_channels, out_channels, kernel_size, n_groups=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv1d(inp_channels, out_channels, kernel_size, padding=kernel_size // 2),\n",
        "            nn.GroupNorm(n_groups, out_channels),\n",
        "            nn.Mish(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "class ConditionalResidualBlock1D(nn.Module):\n",
        "    def __init__(self, \n",
        "            in_channels, \n",
        "            out_channels, \n",
        "            cond_dim,\n",
        "            kernel_size=3,\n",
        "            n_groups=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Conv1dBlock(in_channels, out_channels, kernel_size, n_groups=n_groups),\n",
        "            Conv1dBlock(out_channels, out_channels, kernel_size, n_groups=n_groups),\n",
        "        ])\n",
        "\n",
        "        # FiLM modulation https://arxiv.org/abs/1709.07871\n",
        "        # predicts per-channel scale and bias\n",
        "        cond_channels = out_channels * 2\n",
        "        self.out_channels = out_channels\n",
        "        self.cond_encoder = nn.Sequential(\n",
        "            nn.Mish(),\n",
        "            nn.Linear(cond_dim, cond_channels),\n",
        "            nn.Unflatten(-1, (-1, 1))\n",
        "        )\n",
        "\n",
        "        # make sure dimensions compatible\n",
        "        self.residual_conv = nn.Conv1d(in_channels, out_channels, 1) \\\n",
        "            if in_channels != out_channels else nn.Identity()\n",
        "\n",
        "    def forward(self, x, cond):\n",
        "        '''\n",
        "            x : [ batch_size x in_channels x horizon ]\n",
        "            cond : [ batch_size x cond_dim]\n",
        "\n",
        "            returns:\n",
        "            out : [ batch_size x out_channels x horizon ]\n",
        "        '''\n",
        "        out = self.blocks[0](x)\n",
        "        embed = self.cond_encoder(cond)\n",
        "\n",
        "        embed = embed.reshape(\n",
        "            embed.shape[0], 2, self.out_channels, 1)\n",
        "        scale = embed[:,0,...]\n",
        "        bias = embed[:,1,...]\n",
        "        out = scale * out + bias\n",
        "\n",
        "        out = self.blocks[1](out)\n",
        "        out = out + self.residual_conv(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ConditionalUnet1D(nn.Module):\n",
        "    def __init__(self, \n",
        "        input_dim,\n",
        "        global_cond_dim,\n",
        "        diffusion_step_embed_dim=256,\n",
        "        down_dims=[256,512,1024],\n",
        "        kernel_size=5,\n",
        "        n_groups=8\n",
        "        ):\n",
        "        \"\"\"\n",
        "        input_dim: Dim of actions.\n",
        "        global_cond_dim: Dim of global conditioning applied with FiLM \n",
        "          in addition to diffusion step embedding. This is usually obs_horizon * obs_dim\n",
        "        diffusion_step_embed_dim: Size of positional encoding for diffusion iteration k\n",
        "        down_dims: Channel size for each UNet level. \n",
        "          The length of this array determines numebr of levels.\n",
        "        kernel_size: Conv kernel size\n",
        "        n_groups: Number of groups for GroupNorm\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        all_dims = [input_dim] + list(down_dims)\n",
        "        start_dim = down_dims[0]\n",
        "\n",
        "        dsed = diffusion_step_embed_dim\n",
        "        diffusion_step_encoder = nn.Sequential(\n",
        "            SinusoidalPosEmb(dsed),\n",
        "            nn.Linear(dsed, dsed * 4),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(dsed * 4, dsed),\n",
        "        )\n",
        "        cond_dim = dsed + global_cond_dim\n",
        "\n",
        "        in_out = list(zip(all_dims[:-1], all_dims[1:]))\n",
        "        mid_dim = all_dims[-1]\n",
        "        self.mid_modules = nn.ModuleList([\n",
        "            ConditionalResidualBlock1D(\n",
        "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
        "                kernel_size=kernel_size, n_groups=n_groups\n",
        "            ),\n",
        "            ConditionalResidualBlock1D(\n",
        "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
        "                kernel_size=kernel_size, n_groups=n_groups\n",
        "            ),\n",
        "        ])\n",
        "\n",
        "        down_modules = nn.ModuleList([])\n",
        "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
        "            is_last = ind >= (len(in_out) - 1)\n",
        "            down_modules.append(nn.ModuleList([\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_in, dim_out, cond_dim=cond_dim, \n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_out, dim_out, cond_dim=cond_dim, \n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                Downsample1d(dim_out) if not is_last else nn.Identity()\n",
        "            ]))\n",
        "\n",
        "        up_modules = nn.ModuleList([])\n",
        "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
        "            is_last = ind >= (len(in_out) - 1)\n",
        "            up_modules.append(nn.ModuleList([\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_out*2, dim_in, cond_dim=cond_dim,\n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_in, dim_in, cond_dim=cond_dim,\n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                Upsample1d(dim_in) if not is_last else nn.Identity()\n",
        "            ]))\n",
        "        \n",
        "        final_conv = nn.Sequential(\n",
        "            Conv1dBlock(start_dim, start_dim, kernel_size=kernel_size),\n",
        "            nn.Conv1d(start_dim, input_dim, 1),\n",
        "        )\n",
        "\n",
        "        self.diffusion_step_encoder = diffusion_step_encoder\n",
        "        self.up_modules = up_modules\n",
        "        self.down_modules = down_modules\n",
        "        self.final_conv = final_conv\n",
        "\n",
        "        print(\"number of parameters: {:e}\".format(\n",
        "            sum(p.numel() for p in self.parameters()))\n",
        "        )\n",
        "\n",
        "    def forward(self, \n",
        "            sample: torch.Tensor, \n",
        "            timestep: Union[torch.Tensor, float, int], \n",
        "            global_cond=None):\n",
        "        \"\"\"\n",
        "        x: (B,T,input_dim)\n",
        "        timestep: (B,) or int, diffusion step\n",
        "        global_cond: (B,global_cond_dim)\n",
        "        output: (B,T,input_dim)\n",
        "        \"\"\"\n",
        "        # (B,T,C)\n",
        "        sample = sample.moveaxis(-1,-2)\n",
        "        # (B,C,T)\n",
        "\n",
        "        # 1. time\n",
        "        timesteps = timestep\n",
        "        if not torch.is_tensor(timesteps):\n",
        "            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
        "            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n",
        "        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
        "            timesteps = timesteps[None].to(sample.device)\n",
        "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
        "        timesteps = timesteps.expand(sample.shape[0])\n",
        "\n",
        "        global_feature = self.diffusion_step_encoder(timesteps)\n",
        "\n",
        "        if global_cond is not None:\n",
        "            global_feature = torch.cat([\n",
        "                global_feature, global_cond\n",
        "            ], axis=-1)\n",
        "        \n",
        "        x = sample\n",
        "        h = []\n",
        "        for idx, (resnet, resnet2, downsample) in enumerate(self.down_modules):\n",
        "            x = resnet(x, global_feature)\n",
        "            x = resnet2(x, global_feature)\n",
        "            h.append(x)\n",
        "            x = downsample(x)\n",
        "\n",
        "        for mid_module in self.mid_modules:\n",
        "            x = mid_module(x, global_feature)\n",
        "\n",
        "        for idx, (resnet, resnet2, upsample) in enumerate(self.up_modules):\n",
        "            x = torch.cat((x, h.pop()), dim=1)\n",
        "            x = resnet(x, global_feature)\n",
        "            x = resnet2(x, global_feature)\n",
        "            x = upsample(x)\n",
        "\n",
        "        x = self.final_conv(x)\n",
        "\n",
        "        # (B,C,T)\n",
        "        x = x.moveaxis(-1,-2)\n",
        "        # (B,T,C)\n",
        "        return x\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "X-XRB_g3vsgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Vision Encoder**\n",
        "#@markdown\n",
        "#@markdown Defines helper functions:\n",
        "#@markdown - `get_resnet` to initialize standard ResNet vision encoder\n",
        "#@markdown - `replace_bn_with_gn` to replace all BatchNorm layers with GroupNorm\n",
        "\n",
        "def get_resnet(name:str, weights=None, **kwargs) -> nn.Module:\n",
        "    \"\"\"\n",
        "    name: resnet18, resnet34, resnet50\n",
        "    weights: \"IMAGENET1K_V1\", None\n",
        "    \"\"\"\n",
        "    # Use standard ResNet implementation from torchvision\n",
        "    func = getattr(torchvision.models, name)\n",
        "    resnet = func(weights=weights, **kwargs)\n",
        "\n",
        "    # remove the final fully connected layer\n",
        "    # for resnet18, the output dim should be 512\n",
        "    resnet.fc = torch.nn.Identity()\n",
        "    return resnet\n",
        "\n",
        "\n",
        "def replace_submodules(\n",
        "        root_module: nn.Module, \n",
        "        predicate: Callable[[nn.Module], bool], \n",
        "        func: Callable[[nn.Module], nn.Module]) -> nn.Module:\n",
        "    \"\"\"\n",
        "    Replace all submodules selected by the predicate with\n",
        "    the output of func.\n",
        "\n",
        "    predicate: Return true if the module is to be replaced.\n",
        "    func: Return new module to use.\n",
        "    \"\"\"\n",
        "    if predicate(root_module):\n",
        "        return func(root_module)\n",
        "\n",
        "    bn_list = [k.split('.') for k, m \n",
        "        in root_module.named_modules(remove_duplicate=True) \n",
        "        if predicate(m)]\n",
        "    for *parent, k in bn_list:\n",
        "        parent_module = root_module\n",
        "        if len(parent) > 0:\n",
        "            parent_module = root_module.get_submodule('.'.join(parent))\n",
        "        if isinstance(parent_module, nn.Sequential):\n",
        "            src_module = parent_module[int(k)]\n",
        "        else:\n",
        "            src_module = getattr(parent_module, k)\n",
        "        tgt_module = func(src_module)\n",
        "        if isinstance(parent_module, nn.Sequential):\n",
        "            parent_module[int(k)] = tgt_module\n",
        "        else:\n",
        "            setattr(parent_module, k, tgt_module)\n",
        "    # verify that all modules are replaced\n",
        "    bn_list = [k.split('.') for k, m \n",
        "        in root_module.named_modules(remove_duplicate=True) \n",
        "        if predicate(m)]\n",
        "    assert len(bn_list) == 0\n",
        "    return root_module\n",
        "\n",
        "def replace_bn_with_gn(\n",
        "    root_module: nn.Module, \n",
        "    features_per_group: int=16) -> nn.Module:\n",
        "    \"\"\"\n",
        "    Relace all BatchNorm layers with GroupNorm.\n",
        "    \"\"\"\n",
        "    replace_submodules(\n",
        "        root_module=root_module,\n",
        "        predicate=lambda x: isinstance(x, nn.BatchNorm2d),\n",
        "        func=lambda x: nn.GroupNorm(\n",
        "            num_groups=x.num_features//features_per_group, \n",
        "            num_channels=x.num_features)\n",
        "    )\n",
        "    return root_module\n"
      ],
      "metadata": {
        "id": "yXq4r744aMh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Network Demo**\n",
        "\n",
        "# construct ResNet18 encoder\n",
        "# if you have multiple camera views, use seperate encoder weights for each view.\n",
        "vision_encoder = get_resnet('resnet18')\n",
        "\n",
        "# IMPORTANT!\n",
        "# replace all BatchNorm with GroupNorm to work with EMA\n",
        "# performance will tank if you forget to do this!\n",
        "vision_encoder = replace_bn_with_gn(vision_encoder)\n",
        "\n",
        "# ResNet18 has output dim of 512\n",
        "vision_feature_dim = 512\n",
        "# agent_pos is 2 dimensional\n",
        "lowdim_obs_dim = 2\n",
        "# observation feature has 514 dims in total per step\n",
        "obs_dim = vision_feature_dim + lowdim_obs_dim\n",
        "action_dim = 2\n",
        "\n",
        "# create network object\n",
        "noise_pred_net = ConditionalUnet1D(\n",
        "    input_dim=action_dim,\n",
        "    global_cond_dim=obs_dim*obs_horizon\n",
        ")\n",
        "\n",
        "# the final arch has 2 parts\n",
        "nets = nn.ModuleDict({\n",
        "    'vision_encoder': vision_encoder,\n",
        "    'noise_pred_net': noise_pred_net\n",
        "})\n",
        "\n",
        "# demo\n",
        "with torch.no_grad():\n",
        "    # example inputs\n",
        "    image = torch.zeros((1, obs_horizon,3,96,96))\n",
        "    agent_pos = torch.zeros((1, obs_horizon, 2))\n",
        "    # vision encoder\n",
        "    image_features = nets['vision_encoder'](\n",
        "        image.flatten(end_dim=1))\n",
        "    # (2,512)\n",
        "    image_features = image_features.reshape(*image.shape[:2],-1)\n",
        "    # (1,2,512)\n",
        "    obs = torch.cat([image_features, agent_pos],dim=-1)\n",
        "    # (1,2,514)\n",
        "\n",
        "    noised_action = torch.randn((1, pred_horizon, action_dim))\n",
        "    diffusion_iter = torch.zeros((1,))\n",
        "\n",
        "    # the noise prediction network\n",
        "    # takes noisy action, diffusion iteration and observation as input\n",
        "    # predicts the noise added to action\n",
        "    noise = nets['noise_pred_net'](\n",
        "        sample=noised_action, \n",
        "        timestep=diffusion_iter,\n",
        "        global_cond=obs.flatten(start_dim=1))\n",
        "\n",
        "    # illustration of removing noise \n",
        "    # the actual noise removal is performed by NoiseScheduler \n",
        "    # and is dependent on the diffusion noise schedule\n",
        "    denoised_action = noised_action - noise\n",
        "\n",
        "# for this demo, we use DDPMScheduler with 100 diffusion iterations\n",
        "num_diffusion_iters = 100\n",
        "noise_scheduler = DDPMScheduler(\n",
        "    num_train_timesteps=num_diffusion_iters,\n",
        "    # the choise of beta schedule has big impact on performance\n",
        "    # we found squared cosine works the best\n",
        "    beta_schedule='squaredcos_cap_v2',\n",
        "    # clip output to [-1,1] to improve stability\n",
        "    clip_sample=True,\n",
        "    # our network predicts noise (instead of denoised action)\n",
        "    prediction_type='epsilon'\n",
        ")\n",
        "\n",
        "# device transfer\n",
        "device = torch.device('cuda')\n",
        "_ = nets.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4APZkqh336-M",
        "outputId": "6f63d3a9-52f3-4a8b-ee7e-1b0810fc2276"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 7.994727e+07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Training**\n",
        "#@markdown\n",
        "#@markdown Takes about 2.5 hours. If you don't want to wait, skip to the next cell\n",
        "#@markdown to load pre-trained weights\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "# Exponential Moving Average\n",
        "# accelerates training and improves stability\n",
        "# holds a copy of the model weights\n",
        "ema = EMAModel(\n",
        "    model=nets,\n",
        "    power=0.75)\n",
        "\n",
        "# Standard ADAM optimizer\n",
        "# Note that EMA parametesr are not optimized\n",
        "optimizer = torch.optim.AdamW(\n",
        "    params=nets.parameters(), \n",
        "    lr=1e-4, weight_decay=1e-6)\n",
        "\n",
        "# Cosine LR schedule with linear warmup\n",
        "lr_scheduler = get_scheduler(\n",
        "    name='cosine',\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=500,\n",
        "    num_training_steps=len(dataloader) * num_epochs\n",
        ")\n",
        "\n",
        "with tqdm(range(num_epochs), desc='Epoch') as tglobal:\n",
        "    # epoch loop\n",
        "    for epoch_idx in tglobal:\n",
        "        epoch_loss = list()\n",
        "        # batch loop\n",
        "        with tqdm(dataloader, desc='Batch', leave=False) as tepoch:\n",
        "            for nbatch in tepoch:\n",
        "                # data normalized in dataset\n",
        "                # device transfer\n",
        "                nimage = nbatch['image'][:,:obs_horizon].to(device)\n",
        "                nagent_pos = nbatch['agent_pos'][:,:obs_horizon].to(device)\n",
        "                naction = nbatch['action'].to(device)\n",
        "                B = nagent_pos.shape[0]\n",
        "\n",
        "                # encoder vision features\n",
        "                image_features = nets['vision_encoder'](\n",
        "                    nimage.flatten(end_dim=1))\n",
        "                image_features = image_features.reshape(\n",
        "                    *nimage.shape[:2],-1)\n",
        "                # (B,obs_horizon,D)\n",
        "\n",
        "                # concatenate vision feature and low-dim obs\n",
        "                obs_features = torch.cat([image_features, nagent_pos], dim=-1)\n",
        "                obs_cond = obs_features.flatten(start_dim=1)\n",
        "                # (B, obs_horizon * obs_dim)\n",
        "\n",
        "                # sample noise to add to actions\n",
        "                noise = torch.randn(naction.shape, device=device)\n",
        "\n",
        "                # sample a diffusion iteration for each data point\n",
        "                timesteps = torch.randint(\n",
        "                    0, noise_scheduler.config.num_train_timesteps, \n",
        "                    (B,), device=device\n",
        "                ).long()\n",
        "\n",
        "                # add noise to the clean images according to the noise magnitude at each diffusion iteration\n",
        "                # (this is the forward diffusion process)\n",
        "                noisy_actions = noise_scheduler.add_noise(\n",
        "                    naction, noise, timesteps)\n",
        "                \n",
        "                # predict the noise residual\n",
        "                noise_pred = noise_pred_net(\n",
        "                    noisy_actions, timesteps, global_cond=obs_cond)\n",
        "                \n",
        "                # L2 loss\n",
        "                loss = nn.functional.mse_loss(noise_pred, noise)\n",
        "\n",
        "                # optimize\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                # step lr scheduler every batch\n",
        "                # this is different from standard pytorch behavior\n",
        "                lr_scheduler.step()\n",
        "\n",
        "                # update Exponential Moving Average of the model weights\n",
        "                ema.step(nets)\n",
        "\n",
        "                # logging\n",
        "                loss_cpu = loss.item()\n",
        "                epoch_loss.append(loss_cpu)\n",
        "                tepoch.set_postfix(loss=loss_cpu)\n",
        "        tglobal.set_postfix(loss=np.mean(epoch_loss))\n",
        "\n",
        "# Weights of the EMA model\n",
        "# is used for inference\n",
        "ema_net = ema.averaged_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445,
          "referenced_widgets": [
            "200f515cb4214f4cb520d8f8bc1f36a0",
            "1d78a8564d4945548183e41c7f949e1a",
            "f0989fb473e74cb8ac82b73b1cd9de6c",
            "e14153bfbaf84c79ba274c32abcc775e",
            "bf0322956b4c41e5aaecae47c2724374",
            "9757b998754f4e8398cf336c6dcfa975",
            "740cc90c8dc241a28a497770bae1e0a7",
            "286953f56dbe49ac852b3dac4d34d4eb",
            "c50e8b171f4a4e41baad5bf8a79da122",
            "6f49b866d0f9476995b6f87aabc04cdd",
            "a1d6550c4abe4002a7eed9b0d30be92d",
            "e56398a656924094925ed6da9639af90",
            "0d612e88990e4df280014354e024285a",
            "68c4cf61318943cb88bc155dfe8c6b89",
            "66be6ae60f464dd6b1fe82740b45ae0d",
            "8c88a1a25d79443c83db625da7a7ecb8",
            "0130f6e35418428aad81460152ce3eb6",
            "78eed74360f841ed9a89db3afeaa8577",
            "2ab3d498e2e347f38c12e071fd0ca9d4",
            "2ddab6a6b0bc420f9c7599805a69e0d9",
            "db4cbd2f04ad403fb04a01a9de604d02",
            "25e084e4cb0549bea3269298627ade20"
          ]
        },
        "id": "93E9RdnR4D8v",
        "outputId": "697325c2-aa98-419a-f21c-4a2aec777d5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "200f515cb4214f4cb520d8f8bc1f36a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batch:   0%|          | 0/379 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e56398a656924094925ed6da9639af90"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-484363cd16ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;31m# update Exponential Moving Average of the model weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0mema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise_pred_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;31m# logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/diffusers/training_utils.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, new_model)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mema_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mema_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mema_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                 \u001b[0mema_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mema_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Loading Pretrained Checkpoint**\n",
        "#@markdown Set `load_pretrained = True` to load pretrained weights.\n",
        "\n",
        "load_pretrained = False\n",
        "if load_pretrained:\n",
        "  ckpt_path = \"pusht_vision_100ep.ckpt\"\n",
        "  if not os.path.isfile(ckpt_path):\n",
        "      id = \"1XKpfNSlwYMGaF5CncoFaLKCDTWoLAHf1&confirm=t\"\n",
        "      gdown.download(id=id, output=ckpt_path, quiet=False)\n",
        "\n",
        "  state_dict = torch.load(ckpt_path, map_location='cuda')\n",
        "  ema_nets = nets\n",
        "  ema_nets.load_state_dict(state_dict)\n",
        "  print('Pretrained weights loaded.')\n",
        "else:\n",
        "  print(\"Skipped pretrained weight loading.\")"
      ],
      "metadata": {
        "id": "6F3hUbIuxGdO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9ab5e42-75d0-4d0e-eb3c-b6c8390d3f00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1XKpfNSlwYMGaF5CncoFaLKCDTWoLAHf1&confirm=t\n",
            "To: /content/pusht_vision_100ep.ckpt\n",
            "100%|██████████| 365M/365M [00:05<00:00, 69.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrained weights loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Inference**\n",
        "\n",
        "# limit enviornment interaction to 200 steps before termination\n",
        "max_steps = 200\n",
        "env = PushTImageEnv()\n",
        "# use a seed >200 to avoid initial states seen in the training dataset\n",
        "env.seed(100000)\n",
        "\n",
        "# get first observation\n",
        "obs = env.reset()\n",
        "\n",
        "# keep a queue of last 2 steps of observations\n",
        "obs_deque = collections.deque(\n",
        "    [obs] * obs_horizon, maxlen=obs_horizon)\n",
        "# save visualization and rewards\n",
        "imgs = [env.render(mode='rgb_array')]\n",
        "rewards = list()\n",
        "done = False\n",
        "step_idx = 0\n",
        "\n",
        "with tqdm(total=max_steps, desc=\"Eval PushTImageEnv\") as pbar:\n",
        "    while not done:\n",
        "        B = 1\n",
        "        # stack the last obs_horizon number of observations\n",
        "        images = np.stack([x['image'] for x in obs_deque])\n",
        "        agent_poses = np.stack([x['agent_pos'] for x in obs_deque])\n",
        "\n",
        "        # normalize observation\n",
        "        nagent_poses = normalize_data(agent_poses, stats=stats['agent_pos'])\n",
        "        # images are already normalized to [0,1]\n",
        "        nimages = images\n",
        "\n",
        "        # device transfer\n",
        "        nimages = torch.from_numpy(nimages).to(device, dtype=torch.float32)\n",
        "        # (2,3,96,96)\n",
        "        nagent_poses = torch.from_numpy(nagent_poses).to(device, dtype=torch.float32)\n",
        "        # (2,2)\n",
        "\n",
        "        # infer action\n",
        "        with torch.no_grad():\n",
        "            # get image features\n",
        "            image_features = ema_nets['vision_encoder'](nimages)\n",
        "            # (2,512)\n",
        "\n",
        "            # concat with low-dim observations\n",
        "            obs_features = torch.cat([image_features, nagent_poses], dim=-1)\n",
        "\n",
        "            # reshape observation to (B,obs_horizon*obs_dim)\n",
        "            obs_cond = obs_features.unsqueeze(0).flatten(start_dim=1)\n",
        "\n",
        "            # initialize action from Guassian noise\n",
        "            noisy_action = torch.randn(\n",
        "                (B, pred_horizon, action_dim), device=device)\n",
        "            naction = noisy_action\n",
        "            \n",
        "            # init scheduler\n",
        "            noise_scheduler.set_timesteps(num_diffusion_iters)\n",
        "\n",
        "            for k in noise_scheduler.timesteps:\n",
        "                # predict noise\n",
        "                noise_pred = ema_nets['noise_pred_net'](\n",
        "                    sample=naction, \n",
        "                    timestep=k,\n",
        "                    global_cond=obs_cond\n",
        "                )\n",
        "\n",
        "                # inverse diffusion step (remove noise)\n",
        "                naction = noise_scheduler.step(\n",
        "                    model_output=noise_pred,\n",
        "                    timestep=k,\n",
        "                    sample=naction\n",
        "                ).prev_sample\n",
        "\n",
        "        # unnormalize action\n",
        "        naction = naction.detach().to('cpu').numpy()\n",
        "        # (B, pred_horizon, action_dim)\n",
        "        naction = naction[0]\n",
        "        action_pred = unnormalize_data(naction, stats=stats['action'])\n",
        "\n",
        "        # only take action_horizon number of actions\n",
        "        start = obs_horizon - 1\n",
        "        end = start + action_horizon\n",
        "        action = action_pred[start:end,:]\n",
        "        # (action_horizon, action_dim)\n",
        "\n",
        "        # execute action_horizon number of steps\n",
        "        # without replanning\n",
        "        for i in range(len(action)):\n",
        "            # stepping env\n",
        "            obs, reward, done, info = env.step(action[i])\n",
        "            # save observations\n",
        "            obs_deque.append(obs)\n",
        "            # and reward/vis\n",
        "            rewards.append(reward)\n",
        "            imgs.append(env.render(mode='rgb_array'))\n",
        "\n",
        "            # update progress bar\n",
        "            step_idx += 1\n",
        "            pbar.update(1)\n",
        "            pbar.set_postfix(reward=reward)\n",
        "            if step_idx > max_steps:\n",
        "                done = True\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "# print out the maximum target coverage\n",
        "print('Score: ', max(rewards))\n",
        "\n",
        "# visualize\n",
        "from IPython.display import Video\n",
        "vwrite('vis.mp4', imgs)\n",
        "Video('vis.mp4', embed=True, width=256, height=256)"
      ],
      "metadata": {
        "id": "OyLjlNQk5nr9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327,
          "referenced_widgets": [
            "95c123afc6f84bc7953bb96d0d56a6e3",
            "6759c49d4d5f4882b6829d2699da0034",
            "82031f6f157a43859ab1694fe364c85b",
            "c016aa08f05547c58cfca2dd74ea5672",
            "f3ecb90af134446bb26e1850aaf8c6d3",
            "b8e9ca44f80342889cd919d904ca3606",
            "7b6afa435fa7467bbe0ee2a89426a489",
            "ba900afe5b084ce18c23780a99346f53",
            "a17da852e4104ba681f356a8d2c454aa",
            "e7cfe0c39c80488aba61edbe3e570653",
            "104ecf3a8d0d441580f059ed101286b6"
          ]
        },
        "outputId": "bcfc7b02-92df-43f0-f603-310dcb3d9f72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval PushTImageEnv:   0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "95c123afc6f84bc7953bb96d0d56a6e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score:  1.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Video object>"
            ],
            "text/html": [
              "<video controls  width=\"256\"  height=\"256\">\n",
              " <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n",
              " Your browser does not support the video tag.\n",
              " </video>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}