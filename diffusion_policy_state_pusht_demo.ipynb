{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d3d29f28c9174a29a69857bc0b4a845a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_55d6b6d44b894986bd142a0e5d202278",
              "IPY_MODEL_9110e03d3df7454d96a6faa6f17823a6",
              "IPY_MODEL_61bfe1271ad943fb84cf0a2eb7098357"
            ],
            "layout": "IPY_MODEL_b4da5d7c11194f9fa7313ddf9ec5dc4e"
          }
        },
        "55d6b6d44b894986bd142a0e5d202278": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e055a762b714fd5b5d79804c585b3e3",
            "placeholder": "​",
            "style": "IPY_MODEL_ef7033608015473587210584216fabc9",
            "value": "Eval PushTStateEnv: "
          }
        },
        "9110e03d3df7454d96a6faa6f17823a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b52623ee9c143e29febe6b413469699",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aeaab85bac8c458fa05eb8944d17f94f",
            "value": 200
          }
        },
        "61bfe1271ad943fb84cf0a2eb7098357": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19c8ff9b14ef45148165a5216d724fd3",
            "placeholder": "​",
            "style": "IPY_MODEL_379f06dd98f344a28ccd761601865fc1",
            "value": " 201/? [00:33&lt;00:00,  6.20it/s, reward=0.98]"
          }
        },
        "b4da5d7c11194f9fa7313ddf9ec5dc4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e055a762b714fd5b5d79804c585b3e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef7033608015473587210584216fabc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b52623ee9c143e29febe6b413469699": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aeaab85bac8c458fa05eb8944d17f94f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "19c8ff9b14ef45148165a5216d724fd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "379f06dd98f344a28ccd761601865fc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wrangelvid/diffusion-policy-dev/blob/add-notebooks/diffusion_policy_state_pusht_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Installing pip packages**\n",
        "#@markdown - Diffusion Model: [PyTorch](https://pytorch.org) & [HuggingFace diffusers](https://huggingface.co/docs/diffusers/index)\n",
        "#@markdown - Dataset Loading: [Zarr](https://zarr.readthedocs.io/en/stable/) & numcodecs\n",
        "#@markdown - Push-T Env: gym, pygame, pymunk & shapely\n",
        "!python --version\n",
        "!pip3 uninstall cvxpy -y > /dev/null\n",
        "!pip3 install setuptools==65.5.0 > /dev/null\n",
        "# hack for gym==0.21.0 https://github.com/openai/gym/issues/3176\n",
        "!pip3 install torch==1.13.1 torchvision==0.14.1 diffusers==0.11.1 \\\n",
        "scikit-image==0.19.3 scikit-video==1.1.11 zarr==2.12.0 numcodecs==0.10.2 \\\n",
        "pygame==2.1.2 pymunk==6.2.1 gym==0.21.0 shapely==1.8.4 \\\n",
        "&> /dev/null # mute output"
      ],
      "metadata": {
        "id": "2QwO2gAgiJS2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c814b006-448a-4085-a59d-6ba7c7b73fdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.9.16\n",
            "\u001b[33mWARNING: Skipping cvxpy as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Imports**\n",
        "# diffusion policy import\n",
        "from typing import Tuple, Sequence, Dict, Union, Optional\n",
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import collections\n",
        "import zarr\n",
        "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
        "from diffusers.training_utils import EMAModel\n",
        "from diffusers.optimization import get_scheduler\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# env import\n",
        "import gym\n",
        "from gym import spaces\n",
        "import pygame\n",
        "import pymunk\n",
        "import pymunk.pygame_util\n",
        "from pymunk.space_debug_draw_options import SpaceDebugColor\n",
        "from pymunk.vec2d import Vec2d\n",
        "import shapely.geometry as sg\n",
        "import cv2\n",
        "import skimage.transform as st\n",
        "from skvideo.io import vwrite\n",
        "from IPython.display import Video\n",
        "import gdown\n",
        "import os"
      ],
      "metadata": {
        "id": "VrX4VTl5pYNq",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Environment**\n",
        "#@markdown Defines a PyMunk-based Push-T environment `PushTEnv`.\n",
        "#@markdown \n",
        "#@markdown **Goal**: push the gray T-block into the green area.\n",
        "#@markdown\n",
        "#@markdown Adapted from [Implicit Behavior Cloning](https://implicitbc.github.io/)\n",
        "\n",
        "\n",
        "positive_y_is_up: bool = False\n",
        "\"\"\"Make increasing values of y point upwards.\n",
        "\n",
        "When True::\n",
        "\n",
        "    y\n",
        "    ^\n",
        "    |      . (3, 3)\n",
        "    |\n",
        "    |   . (2, 2)\n",
        "    |\n",
        "    +------ > x\n",
        "    \n",
        "When False::\n",
        "\n",
        "    +------ > x\n",
        "    |\n",
        "    |   . (2, 2)\n",
        "    |\n",
        "    |      . (3, 3)\n",
        "    v\n",
        "    y\n",
        "    \n",
        "\"\"\"\n",
        "\n",
        "def to_pygame(p: Tuple[float, float], surface: pygame.Surface) -> Tuple[int, int]:\n",
        "    \"\"\"Convenience method to convert pymunk coordinates to pygame surface\n",
        "    local coordinates.\n",
        "\n",
        "    Note that in case positive_y_is_up is False, this function wont actually do\n",
        "    anything except converting the point to integers.\n",
        "    \"\"\"\n",
        "    if positive_y_is_up:\n",
        "        return round(p[0]), surface.get_height() - round(p[1])\n",
        "    else:\n",
        "        return round(p[0]), round(p[1])\n",
        "\n",
        "\n",
        "def light_color(color: SpaceDebugColor):\n",
        "    color = np.minimum(1.2 * np.float32([color.r, color.g, color.b, color.a]), np.float32([255]))\n",
        "    color = SpaceDebugColor(r=color[0], g=color[1], b=color[2], a=color[3])\n",
        "    return color\n",
        "\n",
        "class DrawOptions(pymunk.SpaceDebugDrawOptions):\n",
        "    def __init__(self, surface: pygame.Surface) -> None:\n",
        "        \"\"\"Draw a pymunk.Space on a pygame.Surface object.\n",
        "\n",
        "        Typical usage::\n",
        "\n",
        "        >>> import pymunk\n",
        "        >>> surface = pygame.Surface((10,10))\n",
        "        >>> space = pymunk.Space()\n",
        "        >>> options = pymunk.pygame_util.DrawOptions(surface)\n",
        "        >>> space.debug_draw(options)\n",
        "\n",
        "        You can control the color of a shape by setting shape.color to the color\n",
        "        you want it drawn in::\n",
        "\n",
        "        >>> c = pymunk.Circle(None, 10)\n",
        "        >>> c.color = pygame.Color(\"pink\")\n",
        "\n",
        "        See pygame_util.demo.py for a full example\n",
        "\n",
        "        Since pygame uses a coordiante system where y points down (in contrast\n",
        "        to many other cases), you either have to make the physics simulation\n",
        "        with Pymunk also behave in that way, or flip everything when you draw.\n",
        "\n",
        "        The easiest is probably to just make the simulation behave the same\n",
        "        way as Pygame does. In that way all coordinates used are in the same\n",
        "        orientation and easy to reason about::\n",
        "\n",
        "        >>> space = pymunk.Space()\n",
        "        >>> space.gravity = (0, -1000)\n",
        "        >>> body = pymunk.Body()\n",
        "        >>> body.position = (0, 0) # will be positioned in the top left corner\n",
        "        >>> space.debug_draw(options)\n",
        "\n",
        "        To flip the drawing its possible to set the module property\n",
        "        :py:data:`positive_y_is_up` to True. Then the pygame drawing will flip\n",
        "        the simulation upside down before drawing::\n",
        "\n",
        "        >>> positive_y_is_up = True\n",
        "        >>> body = pymunk.Body()\n",
        "        >>> body.position = (0, 0)\n",
        "        >>> # Body will be position in bottom left corner\n",
        "\n",
        "        :Parameters:\n",
        "                surface : pygame.Surface\n",
        "                    Surface that the objects will be drawn on\n",
        "        \"\"\"\n",
        "        self.surface = surface\n",
        "        super(DrawOptions, self).__init__()\n",
        "\n",
        "    def draw_circle(\n",
        "        self,\n",
        "        pos: Vec2d,\n",
        "        angle: float,\n",
        "        radius: float,\n",
        "        outline_color: SpaceDebugColor,\n",
        "        fill_color: SpaceDebugColor,\n",
        "    ) -> None:\n",
        "        p = to_pygame(pos, self.surface)\n",
        "\n",
        "        pygame.draw.circle(self.surface, fill_color.as_int(), p, round(radius), 0)\n",
        "        pygame.draw.circle(self.surface, light_color(fill_color).as_int(), p, round(radius-4), 0)\n",
        "\n",
        "        circle_edge = pos + Vec2d(radius, 0).rotated(angle)\n",
        "        p2 = to_pygame(circle_edge, self.surface)\n",
        "        line_r = 2 if radius > 20 else 1\n",
        "        # pygame.draw.lines(self.surface, outline_color.as_int(), False, [p, p2], line_r)\n",
        "\n",
        "    def draw_segment(self, a: Vec2d, b: Vec2d, color: SpaceDebugColor) -> None:\n",
        "        p1 = to_pygame(a, self.surface)\n",
        "        p2 = to_pygame(b, self.surface)\n",
        "\n",
        "        pygame.draw.aalines(self.surface, color.as_int(), False, [p1, p2])\n",
        "\n",
        "    def draw_fat_segment(\n",
        "        self,\n",
        "        a: Tuple[float, float],\n",
        "        b: Tuple[float, float],\n",
        "        radius: float,\n",
        "        outline_color: SpaceDebugColor,\n",
        "        fill_color: SpaceDebugColor,\n",
        "    ) -> None:\n",
        "        p1 = to_pygame(a, self.surface)\n",
        "        p2 = to_pygame(b, self.surface)\n",
        "\n",
        "        r = round(max(1, radius * 2))\n",
        "        pygame.draw.lines(self.surface, fill_color.as_int(), False, [p1, p2], r)\n",
        "        if r > 2:\n",
        "            orthog = [abs(p2[1] - p1[1]), abs(p2[0] - p1[0])]\n",
        "            if orthog[0] == 0 and orthog[1] == 0:\n",
        "                return\n",
        "            scale = radius / (orthog[0] * orthog[0] + orthog[1] * orthog[1]) ** 0.5\n",
        "            orthog[0] = round(orthog[0] * scale)\n",
        "            orthog[1] = round(orthog[1] * scale)\n",
        "            points = [\n",
        "                (p1[0] - orthog[0], p1[1] - orthog[1]),\n",
        "                (p1[0] + orthog[0], p1[1] + orthog[1]),\n",
        "                (p2[0] + orthog[0], p2[1] + orthog[1]),\n",
        "                (p2[0] - orthog[0], p2[1] - orthog[1]),\n",
        "            ]\n",
        "            pygame.draw.polygon(self.surface, fill_color.as_int(), points)\n",
        "            pygame.draw.circle(\n",
        "                self.surface,\n",
        "                fill_color.as_int(),\n",
        "                (round(p1[0]), round(p1[1])),\n",
        "                round(radius),\n",
        "            )\n",
        "            pygame.draw.circle(\n",
        "                self.surface,\n",
        "                fill_color.as_int(),\n",
        "                (round(p2[0]), round(p2[1])),\n",
        "                round(radius),\n",
        "            )\n",
        "\n",
        "    def draw_polygon(\n",
        "        self,\n",
        "        verts: Sequence[Tuple[float, float]],\n",
        "        radius: float,\n",
        "        outline_color: SpaceDebugColor,\n",
        "        fill_color: SpaceDebugColor,\n",
        "    ) -> None:\n",
        "        ps = [to_pygame(v, self.surface) for v in verts]\n",
        "        ps += [ps[0]]\n",
        "\n",
        "        radius = 2\n",
        "        pygame.draw.polygon(self.surface, light_color(fill_color).as_int(), ps)\n",
        "\n",
        "        if radius > 0:\n",
        "            for i in range(len(verts)):\n",
        "                a = verts[i]\n",
        "                b = verts[(i + 1) % len(verts)]\n",
        "                self.draw_fat_segment(a, b, radius, fill_color, fill_color)\n",
        "\n",
        "    def draw_dot(\n",
        "        self, size: float, pos: Tuple[float, float], color: SpaceDebugColor\n",
        "    ) -> None:\n",
        "        p = to_pygame(pos, self.surface)\n",
        "        pygame.draw.circle(self.surface, color.as_int(), p, round(size), 0)\n",
        "\n",
        "\n",
        "def pymunk_to_shapely(body, shapes):\n",
        "    geoms = list()\n",
        "    for shape in shapes:\n",
        "        if isinstance(shape, pymunk.shapes.Poly):\n",
        "            verts = [body.local_to_world(v) for v in shape.get_vertices()]\n",
        "            verts += [verts[0]]\n",
        "            geoms.append(sg.Polygon(verts))\n",
        "        else:\n",
        "            raise RuntimeError(f'Unsupported shape type {type(shape)}')\n",
        "    geom = sg.MultiPolygon(geoms)\n",
        "    return geom\n",
        "\n",
        "# env\n",
        "class PushTEnv(gym.Env):\n",
        "    metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": 10}\n",
        "    reward_range = (0., 1.)\n",
        "\n",
        "    def __init__(self,\n",
        "            legacy=False, \n",
        "            block_cog=None, damping=None,\n",
        "            render_action=True,\n",
        "            render_size=96,\n",
        "            reset_to_state=None\n",
        "        ):\n",
        "        self._seed = None\n",
        "        self.seed()\n",
        "        self.window_size = ws = 512  # The size of the PyGame window\n",
        "        self.render_size = render_size\n",
        "        self.sim_hz = 100\n",
        "        # Local controller params.\n",
        "        self.k_p, self.k_v = 100, 20    # PD control.z\n",
        "        self.control_hz = self.metadata['video.frames_per_second']\n",
        "        # legcay set_state for data compatiblity\n",
        "        self.legacy = legacy\n",
        "\n",
        "        # agent_pos, block_pos, block_angle\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.array([0,0,0,0,0], dtype=np.float64),\n",
        "            high=np.array([ws,ws,ws,ws,np.pi*2], dtype=np.float64),\n",
        "            shape=(5,),\n",
        "            dtype=np.float64\n",
        "        )\n",
        "\n",
        "        # positional goal for agent\n",
        "        self.action_space = spaces.Box(\n",
        "            low=np.array([0,0], dtype=np.float64),\n",
        "            high=np.array([ws,ws], dtype=np.float64),\n",
        "            shape=(2,),\n",
        "            dtype=np.float64\n",
        "        )\n",
        "\n",
        "        self.block_cog = block_cog\n",
        "        self.damping = damping\n",
        "        self.render_action = render_action\n",
        "\n",
        "        \"\"\"\n",
        "        If human-rendering is used, `self.window` will be a reference\n",
        "        to the window that we draw to. `self.clock` will be a clock that is used\n",
        "        to ensure that the environment is rendered at the correct framerate in\n",
        "        human-mode. They will remain `None` until human-mode is used for the\n",
        "        first time.\n",
        "        \"\"\"\n",
        "        self.window = None\n",
        "        self.clock = None\n",
        "        self.screen = None\n",
        "\n",
        "        self.space = None\n",
        "        self.teleop = None\n",
        "        self.render_buffer = None\n",
        "        self.latest_action = None\n",
        "        self.reset_to_state = reset_to_state\n",
        "    \n",
        "    def reset(self):\n",
        "        seed = self._seed\n",
        "        self._setup()\n",
        "        if self.block_cog is not None:\n",
        "            self.block.center_of_gravity = self.block_cog\n",
        "        if self.damping is not None:\n",
        "            self.space.damping = self.damping\n",
        "        \n",
        "        # use legacy RandomState for compatiblity\n",
        "        state = self.reset_to_state\n",
        "        if state is None:\n",
        "            rs = np.random.RandomState(seed=seed)\n",
        "            state = np.array([\n",
        "                rs.randint(50, 450), rs.randint(50, 450),\n",
        "                rs.randint(100, 400), rs.randint(100, 400),\n",
        "                rs.randn() * 2 * np.pi - np.pi\n",
        "                ])\n",
        "        self._set_state(state)\n",
        "\n",
        "        observation = self._get_obs()\n",
        "        return observation\n",
        "\n",
        "    def step(self, action):\n",
        "        dt = 1.0 / self.sim_hz\n",
        "        self.n_contact_points = 0\n",
        "        n_steps = self.sim_hz // self.control_hz\n",
        "        if action is not None:\n",
        "            self.latest_action = action\n",
        "            for i in range(n_steps):\n",
        "                # Step PD control.\n",
        "                # self.agent.velocity = self.k_p * (act - self.agent.position)    # P control works too.\n",
        "                acceleration = self.k_p * (action - self.agent.position) + self.k_v * (Vec2d(0, 0) - self.agent.velocity)\n",
        "                self.agent.velocity += acceleration * dt\n",
        "\n",
        "                # Step physics.\n",
        "                self.space.step(dt)\n",
        "\n",
        "        # compute reward\n",
        "        goal_body = self._get_goal_pose_body(self.goal_pose)\n",
        "        goal_geom = pymunk_to_shapely(goal_body, self.block.shapes)\n",
        "        block_geom = pymunk_to_shapely(self.block, self.block.shapes)\n",
        "\n",
        "        intersection_area = goal_geom.intersection(block_geom).area\n",
        "        goal_area = goal_geom.area\n",
        "        coverage = intersection_area / goal_area\n",
        "        reward = np.clip(coverage / self.success_threshold, 0, 1)\n",
        "        done = coverage > self.success_threshold\n",
        "\n",
        "        observation = self._get_obs()\n",
        "        info = self._get_info()\n",
        "\n",
        "        return observation, reward, done, info\n",
        "\n",
        "    def render(self, mode):\n",
        "        return self._render_frame(mode)\n",
        "\n",
        "    def teleop_agent(self):\n",
        "        TeleopAgent = collections.namedtuple('TeleopAgent', ['act'])\n",
        "        def act(obs):\n",
        "            act = None\n",
        "            mouse_position = pymunk.pygame_util.from_pygame(Vec2d(*pygame.mouse.get_pos()), self.screen)\n",
        "            if self.teleop or (mouse_position - self.agent.position).length < 30:\n",
        "                self.teleop = True\n",
        "                act = mouse_position\n",
        "            return act\n",
        "        return TeleopAgent(act)\n",
        "\n",
        "    def _get_obs(self):\n",
        "        obs = np.array(\n",
        "            tuple(self.agent.position) \\\n",
        "            + tuple(self.block.position) \\\n",
        "            + (self.block.angle % (2 * np.pi),))\n",
        "        return obs\n",
        "\n",
        "    def _get_goal_pose_body(self, pose):\n",
        "        mass = 1\n",
        "        inertia = pymunk.moment_for_box(mass, (50, 100))\n",
        "        body = pymunk.Body(mass, inertia)\n",
        "        # preserving the legacy assignment order for compatibility\n",
        "        # the order here dosn't matter somehow, maybe because CoM is aligned with body origin\n",
        "        body.position = pose[:2].tolist()\n",
        "        body.angle = pose[2]\n",
        "        return body\n",
        "    \n",
        "    def _get_info(self):\n",
        "        n_steps = self.sim_hz // self.control_hz\n",
        "        n_contact_points_per_step = int(np.ceil(self.n_contact_points / n_steps))\n",
        "        info = {\n",
        "            'pos_agent': np.array(self.agent.position),\n",
        "            'vel_agent': np.array(self.agent.velocity),\n",
        "            'block_pose': np.array(list(self.block.position) + [self.block.angle]),\n",
        "            'goal_pose': self.goal_pose,\n",
        "            'n_contacts': n_contact_points_per_step}\n",
        "        return info\n",
        "\n",
        "    def _render_frame(self, mode):\n",
        "\n",
        "        if self.window is None and mode == \"human\":\n",
        "            pygame.init()\n",
        "            pygame.display.init()\n",
        "            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
        "        if self.clock is None and mode == \"human\":\n",
        "            self.clock = pygame.time.Clock()\n",
        "\n",
        "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
        "        canvas.fill((255, 255, 255))\n",
        "        self.screen = canvas\n",
        "\n",
        "        draw_options = DrawOptions(canvas)\n",
        "\n",
        "        # Draw goal pose.\n",
        "        goal_body = self._get_goal_pose_body(self.goal_pose)\n",
        "        for shape in self.block.shapes:\n",
        "            goal_points = [pymunk.pygame_util.to_pygame(goal_body.local_to_world(v), draw_options.surface) for v in shape.get_vertices()]\n",
        "            goal_points += [goal_points[0]]\n",
        "            pygame.draw.polygon(canvas, self.goal_color, goal_points)\n",
        "\n",
        "        # Draw agent and block.\n",
        "        self.space.debug_draw(draw_options)\n",
        "\n",
        "        if mode == \"human\":\n",
        "            # The following line copies our drawings from `canvas` to the visible window\n",
        "            self.window.blit(canvas, canvas.get_rect())\n",
        "            pygame.event.pump()\n",
        "            pygame.display.update()\n",
        "\n",
        "            # the clock is aleady ticked during in step for \"human\"\n",
        "\n",
        "\n",
        "        img = np.transpose(\n",
        "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
        "            )\n",
        "        img = cv2.resize(img, (self.render_size, self.render_size))\n",
        "        if self.render_action:\n",
        "            if self.render_action and (self.latest_action is not None):\n",
        "                action = np.array(self.latest_action)\n",
        "                coord = (action / 512 * 96).astype(np.int32)\n",
        "                marker_size = int(8/96*self.render_size)\n",
        "                thickness = int(1/96*self.render_size)\n",
        "                cv2.drawMarker(img, coord,\n",
        "                    color=(255,0,0), markerType=cv2.MARKER_CROSS,\n",
        "                    markerSize=marker_size, thickness=thickness)\n",
        "        return img\n",
        "\n",
        "\n",
        "    def close(self):\n",
        "        if self.window is not None:\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()\n",
        "    \n",
        "    def seed(self, seed=None):\n",
        "        if seed is None:\n",
        "            seed = np.random.randint(0,25536)\n",
        "        self._seed = seed\n",
        "        self.np_random = np.random.default_rng(seed)\n",
        "\n",
        "    def _handle_collision(self, arbiter, space, data):\n",
        "        self.n_contact_points += len(arbiter.contact_point_set.points)\n",
        "\n",
        "    def _set_state(self, state):\n",
        "        if isinstance(state, np.ndarray):\n",
        "            state = state.tolist()\n",
        "        pos_agent = state[:2]\n",
        "        pos_block = state[2:4]\n",
        "        rot_block = state[4]\n",
        "        self.agent.position = pos_agent\n",
        "        # setting angle rotates with respect to center of mass\n",
        "        # therefore will modify the geometric position\n",
        "        # if not the same as CoM\n",
        "        # therefore should be modified first.\n",
        "        if self.legacy:\n",
        "            # for compatiblity with legacy data\n",
        "            self.block.position = pos_block\n",
        "            self.block.angle = rot_block\n",
        "        else:\n",
        "            self.block.angle = rot_block\n",
        "            self.block.position = pos_block\n",
        "\n",
        "        # Run physics to take effect\n",
        "        self.space.step(1.0 / self.sim_hz)\n",
        "    \n",
        "    def _set_state_local(self, state_local):\n",
        "        agent_pos_local = state_local[:2]\n",
        "        block_pose_local = state_local[2:]\n",
        "        tf_img_obj = st.AffineTransform(\n",
        "            translation=self.goal_pose[:2], \n",
        "            rotation=self.goal_pose[2])\n",
        "        tf_obj_new = st.AffineTransform(\n",
        "            translation=block_pose_local[:2],\n",
        "            rotation=block_pose_local[2]\n",
        "        )\n",
        "        tf_img_new = st.AffineTransform(\n",
        "            matrix=tf_img_obj.params @ tf_obj_new.params\n",
        "        )\n",
        "        agent_pos_new = tf_img_new(agent_pos_local)\n",
        "        new_state = np.array(\n",
        "            list(agent_pos_new[0]) + list(tf_img_new.translation) \\\n",
        "                + [tf_img_new.rotation])\n",
        "        self._set_state(new_state)\n",
        "        return new_state\n",
        "\n",
        "    def _setup(self):\n",
        "        self.space = pymunk.Space()\n",
        "        self.space.gravity = 0, 0\n",
        "        self.space.damping = 0\n",
        "        self.teleop = False\n",
        "        self.render_buffer = list()\n",
        "        \n",
        "        # Add walls.\n",
        "        walls = [\n",
        "            self._add_segment((5, 506), (5, 5), 2),\n",
        "            self._add_segment((5, 5), (506, 5), 2),\n",
        "            self._add_segment((506, 5), (506, 506), 2),\n",
        "            self._add_segment((5, 506), (506, 506), 2)\n",
        "        ]\n",
        "        self.space.add(*walls)\n",
        "\n",
        "        # Add agent, block, and goal zone.\n",
        "        self.agent = self.add_circle((256, 400), 15)\n",
        "        self.block = self.add_tee((256, 300), 0)\n",
        "        self.goal_color = pygame.Color('LightGreen')\n",
        "        self.goal_pose = np.array([256,256,np.pi/4])  # x, y, theta (in radians)\n",
        "\n",
        "        # Add collision handeling\n",
        "        self.collision_handeler = self.space.add_collision_handler(0, 0)\n",
        "        self.collision_handeler.post_solve = self._handle_collision\n",
        "        self.n_contact_points = 0\n",
        "\n",
        "        self.max_score = 50 * 100\n",
        "        self.success_threshold = 0.95    # 95% coverage.\n",
        "\n",
        "    def _add_segment(self, a, b, radius):\n",
        "        shape = pymunk.Segment(self.space.static_body, a, b, radius)\n",
        "        shape.color = pygame.Color('LightGray')    # https://htmlcolorcodes.com/color-names\n",
        "        return shape\n",
        "\n",
        "    def add_circle(self, position, radius):\n",
        "        body = pymunk.Body(body_type=pymunk.Body.KINEMATIC)\n",
        "        body.position = position\n",
        "        body.friction = 1\n",
        "        shape = pymunk.Circle(body, radius)\n",
        "        shape.color = pygame.Color('RoyalBlue')\n",
        "        self.space.add(body, shape)\n",
        "        return body\n",
        "\n",
        "    def add_box(self, position, height, width):\n",
        "        mass = 1\n",
        "        inertia = pymunk.moment_for_box(mass, (height, width))\n",
        "        body = pymunk.Body(mass, inertia)\n",
        "        body.position = position\n",
        "        shape = pymunk.Poly.create_box(body, (height, width))\n",
        "        shape.color = pygame.Color('LightSlateGray')\n",
        "        self.space.add(body, shape)\n",
        "        return body\n",
        "\n",
        "    def add_tee(self, position, angle, scale=30, color='LightSlateGray', mask=pymunk.ShapeFilter.ALL_MASKS()):\n",
        "        mass = 1\n",
        "        length = 4\n",
        "        vertices1 = [(-length*scale/2, scale),\n",
        "                                 ( length*scale/2, scale),\n",
        "                                 ( length*scale/2, 0),\n",
        "                                 (-length*scale/2, 0)]\n",
        "        inertia1 = pymunk.moment_for_poly(mass, vertices=vertices1)\n",
        "        vertices2 = [(-scale/2, scale),\n",
        "                                 (-scale/2, length*scale),\n",
        "                                 ( scale/2, length*scale),\n",
        "                                 ( scale/2, scale)]\n",
        "        inertia2 = pymunk.moment_for_poly(mass, vertices=vertices1)\n",
        "        body = pymunk.Body(mass, inertia1 + inertia2)\n",
        "        shape1 = pymunk.Poly(body, vertices1)\n",
        "        shape2 = pymunk.Poly(body, vertices2)\n",
        "        shape1.color = pygame.Color(color)\n",
        "        shape2.color = pygame.Color(color)\n",
        "        shape1.filter = pymunk.ShapeFilter(mask=mask)\n",
        "        shape2.filter = pymunk.ShapeFilter(mask=mask)\n",
        "        body.center_of_gravity = (shape1.center_of_gravity + shape2.center_of_gravity) / 2\n",
        "        body.position = position\n",
        "        body.angle = angle\n",
        "        body.friction = 1\n",
        "        self.space.add(body, shape1, shape2)\n",
        "        return body\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "L5E-nR6ornyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Env Demo**\n",
        "#@markdown Standard Gym Env (0.21.0 API)\n",
        "\n",
        "# 0. create env object\n",
        "env = PushTEnv()\n",
        "\n",
        "# 1. seed env for initial state. \n",
        "# Seed 0-200 are used for the demonstration dataset.\n",
        "env.seed(1000)\n",
        "\n",
        "# 2. must reset before use\n",
        "obs = env.reset()\n",
        "\n",
        "# 3. 2D positional action space [0,512]\n",
        "action = env.action_space.sample()\n",
        "\n",
        "# 4. Standard gym step method\n",
        "obs, reward, done, info = env.step(action)\n",
        "\n",
        "# prints and explains each dimension of the observation and action vectors\n",
        "with np.printoptions(precision=4, suppress=True, threshold=5):\n",
        "    print(\"Obs: \", repr(obs))\n",
        "    print(\"Obs:        [agent_x,  agent_y,  block_x,  block_y,    block_angle]\")\n",
        "    print(\"Action: \", repr(action))\n",
        "    print(\"Action:   [target_agent_x, target_agent_y]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OknH8Qfqrtc9",
        "outputId": "badf2845-4864-4df6-90ea-bd4fb94c6fa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obs:  array([126.1474, 160.4223, 292.    , 351.    ,   2.9196])\n",
            "Obs:        [agent_x,  agent_y,  block_x,  block_y,    block_angle]\n",
            "Action:  array([100.295 , 254.3311])\n",
            "Action:   [target_agent_x, target_agent_y]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Dataset**\n",
        "#@markdown\n",
        "#@markdown Defines `PushTStateDataset` and helper functions\n",
        "#@markdown\n",
        "#@markdown The dataset class\n",
        "#@markdown - Load data (obs, action) from a zarr storage\n",
        "#@markdown - Normalizes each dimension of obs and action to [-1,1]\n",
        "#@markdown - Returns\n",
        "#@markdown  - All possible segments with length `pred_horizon`\n",
        "#@markdown  - Pads the beginning and the end of each episode with repetition\n",
        "#@markdown  - key `obs`: shape (obs_horizon, obs_dim)\n",
        "#@markdown  - key `action`: shape (pred_horizon, action_dim) \n",
        "\n",
        "def create_sample_indices(\n",
        "        episode_ends:np.ndarray, sequence_length:int, \n",
        "        pad_before: int=0, pad_after: int=0):\n",
        "    indices = list()\n",
        "    for i in range(len(episode_ends)):\n",
        "        start_idx = 0\n",
        "        if i > 0:\n",
        "            start_idx = episode_ends[i-1]\n",
        "        end_idx = episode_ends[i]\n",
        "        episode_length = end_idx - start_idx\n",
        "        \n",
        "        min_start = -pad_before\n",
        "        max_start = episode_length - sequence_length + pad_after\n",
        "        \n",
        "        # range stops one idx before end\n",
        "        for idx in range(min_start, max_start+1):\n",
        "            buffer_start_idx = max(idx, 0) + start_idx\n",
        "            buffer_end_idx = min(idx+sequence_length, episode_length) + start_idx\n",
        "            start_offset = buffer_start_idx - (idx+start_idx)\n",
        "            end_offset = (idx+sequence_length+start_idx) - buffer_end_idx\n",
        "            sample_start_idx = 0 + start_offset\n",
        "            sample_end_idx = sequence_length - end_offset\n",
        "            indices.append([\n",
        "                buffer_start_idx, buffer_end_idx, \n",
        "                sample_start_idx, sample_end_idx])\n",
        "    indices = np.array(indices)\n",
        "    return indices\n",
        "\n",
        "\n",
        "def sample_sequence(train_data, sequence_length,\n",
        "                    buffer_start_idx, buffer_end_idx, \n",
        "                    sample_start_idx, sample_end_idx):\n",
        "    result = dict()\n",
        "    for key, input_arr in train_data.items():\n",
        "        sample = input_arr[buffer_start_idx:buffer_end_idx]\n",
        "        data = sample\n",
        "        if (sample_start_idx > 0) or (sample_end_idx < sequence_length):\n",
        "            data = np.zeros(\n",
        "                shape=(sequence_length,) + input_arr.shape[1:],\n",
        "                dtype=input_arr.dtype)\n",
        "            if sample_start_idx > 0:\n",
        "                data[:sample_start_idx] = sample[0]\n",
        "            if sample_end_idx < sequence_length:\n",
        "                data[sample_end_idx:] = sample[-1]\n",
        "            data[sample_start_idx:sample_end_idx] = sample\n",
        "        result[key] = data\n",
        "    return result\n",
        "\n",
        "# normalize data\n",
        "def get_data_stats(data):\n",
        "    data = data.reshape(-1,data.shape[-1])\n",
        "    stats = {\n",
        "        'min': np.min(data, axis=0),\n",
        "        'max': np.max(data, axis=0)\n",
        "    }\n",
        "    return stats\n",
        "\n",
        "def normalize_data(data, stats):\n",
        "    # nomalize to [0,1]\n",
        "    ndata = (data - stats['min']) / (stats['max'] - stats['min'])\n",
        "    # normalize to [-1, 1]\n",
        "    ndata = ndata * 2 - 1\n",
        "    return ndata\n",
        "\n",
        "def unnormalize_data(ndata, stats):\n",
        "    ndata = (ndata + 1) / 2\n",
        "    data = ndata * (stats['max'] - stats['min']) + stats['min']\n",
        "    return data\n",
        "\n",
        "# dataset\n",
        "class PushTStateDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset_path,\n",
        "                 pred_horizon, obs_horizon, action_horizon):\n",
        "        \n",
        "        # read from zarr dataset\n",
        "        dataset_root = zarr.open(dataset_path, 'r')\n",
        "        # All demonstration episodes are concatinated in the first dimension N\n",
        "        train_data = {\n",
        "            # (N, action_dim)\n",
        "            'action': dataset_root['data']['action'][:],\n",
        "            # (N, obs_dim)\n",
        "            'obs': dataset_root['data']['state'][:]\n",
        "        }\n",
        "        # Marks one-past the last index for each episode\n",
        "        episode_ends = dataset_root['meta']['episode_ends'][:]\n",
        "        \n",
        "        # compute start and end of each state-action sequence\n",
        "        # also handles padding\n",
        "        indices = create_sample_indices(\n",
        "            episode_ends=episode_ends,\n",
        "            sequence_length=pred_horizon,\n",
        "            # add padding such that each timestep in the dataset are seen\n",
        "            pad_before=obs_horizon-1,\n",
        "            pad_after=action_horizon-1)\n",
        "\n",
        "        # compute statistics and normalized data to [-1,1]\n",
        "        stats = dict()\n",
        "        normalized_train_data = dict()\n",
        "        for key, data in train_data.items():\n",
        "            stats[key] = get_data_stats(data)\n",
        "            normalized_train_data[key] = normalize_data(data, stats[key])\n",
        "\n",
        "        self.indices = indices\n",
        "        self.stats = stats\n",
        "        self.normalized_train_data = normalized_train_data\n",
        "        self.pred_horizon = pred_horizon\n",
        "        self.action_horizon = action_horizon\n",
        "        self.obs_horizon = obs_horizon\n",
        "    \n",
        "    def __len__(self):\n",
        "        # all possible segments of the dataset\n",
        "        return len(self.indices)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # get the start/end indices for this datapoint\n",
        "        buffer_start_idx, buffer_end_idx, \\\n",
        "            sample_start_idx, sample_end_idx = self.indices[idx]\n",
        "\n",
        "        # get nomralized data using these indices\n",
        "        nsample = sample_sequence(\n",
        "            train_data=self.normalized_train_data,\n",
        "            sequence_length=self.pred_horizon,\n",
        "            buffer_start_idx=buffer_start_idx,\n",
        "            buffer_end_idx=buffer_end_idx,\n",
        "            sample_start_idx=sample_start_idx,\n",
        "            sample_end_idx=sample_end_idx\n",
        "        )\n",
        "\n",
        "        # discard unused observations\n",
        "        nsample['obs'] = nsample['obs'][:self.obs_horizon,:]\n",
        "        return nsample\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vHepJOFBucwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Dataset Demo**\n",
        "\n",
        "# download demonstration data from Google Drive\n",
        "dataset_path = \"pusht_cchi_v7_replay.zarr.zip\"\n",
        "if not os.path.isfile(dataset_path):\n",
        "    id = \"1KY1InLurpMvJDRb14L9NlXT_fEsCvVUq&confirm=t\"\n",
        "    gdown.download(id=id, output=dataset_path, quiet=False)\n",
        "\n",
        "# parameters\n",
        "pred_horizon = 16\n",
        "obs_horizon = 2\n",
        "action_horizon = 8\n",
        "#|o|o|                             observations: 2\n",
        "#| |a|a|a|a|a|a|a|a|               actions executed: 8\n",
        "#|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p| actions predicted: 16\n",
        "\n",
        "# create dataset from file\n",
        "dataset = PushTStateDataset(\n",
        "    dataset_path=dataset_path,\n",
        "    pred_horizon=pred_horizon,\n",
        "    obs_horizon=obs_horizon,\n",
        "    action_horizon=action_horizon\n",
        ")\n",
        "# save training data statistics (min, max) for each dim\n",
        "stats = dataset.stats\n",
        "\n",
        "# create dataloader\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=256,\n",
        "    num_workers=1,\n",
        "    shuffle=True,\n",
        "    # accelerate cpu-gpu transfer\n",
        "    pin_memory=True, \n",
        "    # don't kill worker process afte each epoch\n",
        "    persistent_workers=True \n",
        ")\n",
        "\n",
        "# visualize data in batch\n",
        "batch = next(iter(dataloader))\n",
        "print(\"batch['obs'].shape:\", batch['obs'].shape)\n",
        "print(\"batch['action'].shape\", batch['action'].shape)"
      ],
      "metadata": {
        "id": "9ZiHF3lzvB6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21996608-1cea-486c-dea3-0af62c78a499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1KY1InLurpMvJDRb14L9NlXT_fEsCvVUq&confirm=t\n",
            "To: /content/pusht_cchi_v7_replay.zarr.zip\n",
            "100%|██████████| 31.1M/31.1M [00:00<00:00, 59.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch['obs'].shape: torch.Size([256, 2, 5])\n",
            "batch['action'].shape torch.Size([256, 16, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Network**\n",
        "#@markdown\n",
        "#@markdown Defines a 1D UNet architecture `ConditionalUnet1D`\n",
        "#@markdown as the noies prediction network\n",
        "#@markdown\n",
        "#@markdown Components\n",
        "#@markdown - `SinusoidalPosEmb` Positional encoding for the diffusion iteration k\n",
        "#@markdown - `Downsample1d` Strided convolution to reduce temporal resolution\n",
        "#@markdown - `Upsample1d` Transposed convolution to increase temporal resolution\n",
        "#@markdown - `Conv1dBlock` Conv1d --> GroupNorm --> Mish\n",
        "#@markdown - `ConditionalResidualBlock1D` Takes two inputs `x` and `cond`. \\\n",
        "#@markdown `x` is passed through 2 `Conv1dBlock` stacked together with residual connection. \n",
        "#@markdown `cond` is applied to `x` with [FiLM](https://arxiv.org/abs/1709.07871) conditioning.\n",
        "\n",
        "class SinusoidalPosEmb(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        device = x.device\n",
        "        half_dim = self.dim // 2\n",
        "        emb = math.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
        "        emb = x[:, None] * emb[None, :]\n",
        "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
        "        return emb\n",
        "\n",
        "\n",
        "class Downsample1d(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv1d(dim, dim, 3, 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class Upsample1d(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.conv = nn.ConvTranspose1d(dim, dim, 4, 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class Conv1dBlock(nn.Module):\n",
        "    '''\n",
        "        Conv1d --> GroupNorm --> Mish\n",
        "    '''\n",
        "\n",
        "    def __init__(self, inp_channels, out_channels, kernel_size, n_groups=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv1d(inp_channels, out_channels, kernel_size, padding=kernel_size // 2),\n",
        "            nn.GroupNorm(n_groups, out_channels),\n",
        "            nn.Mish(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "class ConditionalResidualBlock1D(nn.Module):\n",
        "    def __init__(self, \n",
        "            in_channels, \n",
        "            out_channels, \n",
        "            cond_dim,\n",
        "            kernel_size=3,\n",
        "            n_groups=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Conv1dBlock(in_channels, out_channels, kernel_size, n_groups=n_groups),\n",
        "            Conv1dBlock(out_channels, out_channels, kernel_size, n_groups=n_groups),\n",
        "        ])\n",
        "\n",
        "        # FiLM modulation https://arxiv.org/abs/1709.07871\n",
        "        # predicts per-channel scale and bias\n",
        "        cond_channels = out_channels * 2\n",
        "        self.out_channels = out_channels\n",
        "        self.cond_encoder = nn.Sequential(\n",
        "            nn.Mish(),\n",
        "            nn.Linear(cond_dim, cond_channels),\n",
        "            nn.Unflatten(-1, (-1, 1))\n",
        "        )\n",
        "\n",
        "        # make sure dimensions compatible\n",
        "        self.residual_conv = nn.Conv1d(in_channels, out_channels, 1) \\\n",
        "            if in_channels != out_channels else nn.Identity()\n",
        "\n",
        "    def forward(self, x, cond):\n",
        "        '''\n",
        "            x : [ batch_size x in_channels x horizon ]\n",
        "            cond : [ batch_size x cond_dim]\n",
        "\n",
        "            returns:\n",
        "            out : [ batch_size x out_channels x horizon ]\n",
        "        '''\n",
        "        out = self.blocks[0](x)\n",
        "        embed = self.cond_encoder(cond)\n",
        "\n",
        "        embed = embed.reshape(\n",
        "            embed.shape[0], 2, self.out_channels, 1)\n",
        "        scale = embed[:,0,...]\n",
        "        bias = embed[:,1,...]\n",
        "        out = scale * out + bias\n",
        "\n",
        "        out = self.blocks[1](out)\n",
        "        out = out + self.residual_conv(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ConditionalUnet1D(nn.Module):\n",
        "    def __init__(self, \n",
        "        input_dim,\n",
        "        global_cond_dim,\n",
        "        diffusion_step_embed_dim=256,\n",
        "        down_dims=[256,512,1024],\n",
        "        kernel_size=5,\n",
        "        n_groups=8\n",
        "        ):\n",
        "        \"\"\"\n",
        "        input_dim: Dim of actions.\n",
        "        global_cond_dim: Dim of global conditioning applied with FiLM \n",
        "          in addition to diffusion step embedding. This is usually obs_horizon * obs_dim\n",
        "        diffusion_step_embed_dim: Size of positional encoding for diffusion iteration k\n",
        "        down_dims: Channel size for each UNet level. \n",
        "          The length of this array determines numebr of levels.\n",
        "        kernel_size: Conv kernel size\n",
        "        n_groups: Number of groups for GroupNorm\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        all_dims = [input_dim] + list(down_dims)\n",
        "        start_dim = down_dims[0]\n",
        "\n",
        "        dsed = diffusion_step_embed_dim\n",
        "        diffusion_step_encoder = nn.Sequential(\n",
        "            SinusoidalPosEmb(dsed),\n",
        "            nn.Linear(dsed, dsed * 4),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(dsed * 4, dsed),\n",
        "        )\n",
        "        cond_dim = dsed + global_cond_dim\n",
        "\n",
        "        in_out = list(zip(all_dims[:-1], all_dims[1:]))\n",
        "        mid_dim = all_dims[-1]\n",
        "        self.mid_modules = nn.ModuleList([\n",
        "            ConditionalResidualBlock1D(\n",
        "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
        "                kernel_size=kernel_size, n_groups=n_groups\n",
        "            ),\n",
        "            ConditionalResidualBlock1D(\n",
        "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
        "                kernel_size=kernel_size, n_groups=n_groups\n",
        "            ),\n",
        "        ])\n",
        "\n",
        "        down_modules = nn.ModuleList([])\n",
        "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
        "            is_last = ind >= (len(in_out) - 1)\n",
        "            down_modules.append(nn.ModuleList([\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_in, dim_out, cond_dim=cond_dim, \n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_out, dim_out, cond_dim=cond_dim, \n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                Downsample1d(dim_out) if not is_last else nn.Identity()\n",
        "            ]))\n",
        "\n",
        "        up_modules = nn.ModuleList([])\n",
        "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
        "            is_last = ind >= (len(in_out) - 1)\n",
        "            up_modules.append(nn.ModuleList([\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_out*2, dim_in, cond_dim=cond_dim,\n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_in, dim_in, cond_dim=cond_dim,\n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                Upsample1d(dim_in) if not is_last else nn.Identity()\n",
        "            ]))\n",
        "        \n",
        "        final_conv = nn.Sequential(\n",
        "            Conv1dBlock(start_dim, start_dim, kernel_size=kernel_size),\n",
        "            nn.Conv1d(start_dim, input_dim, 1),\n",
        "        )\n",
        "\n",
        "        self.diffusion_step_encoder = diffusion_step_encoder\n",
        "        self.up_modules = up_modules\n",
        "        self.down_modules = down_modules\n",
        "        self.final_conv = final_conv\n",
        "\n",
        "        print(\"number of parameters: {:e}\".format(\n",
        "            sum(p.numel() for p in self.parameters()))\n",
        "        )\n",
        "\n",
        "    def forward(self, \n",
        "            sample: torch.Tensor, \n",
        "            timestep: Union[torch.Tensor, float, int], \n",
        "            global_cond=None):\n",
        "        \"\"\"\n",
        "        x: (B,T,input_dim)\n",
        "        timestep: (B,) or int, diffusion step\n",
        "        global_cond: (B,global_cond_dim)\n",
        "        output: (B,T,input_dim)\n",
        "        \"\"\"\n",
        "        # (B,T,C)\n",
        "        sample = sample.moveaxis(-1,-2)\n",
        "        # (B,C,T)\n",
        "\n",
        "        # 1. time\n",
        "        timesteps = timestep\n",
        "        if not torch.is_tensor(timesteps):\n",
        "            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
        "            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n",
        "        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
        "            timesteps = timesteps[None].to(sample.device)\n",
        "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
        "        timesteps = timesteps.expand(sample.shape[0])\n",
        "\n",
        "        global_feature = self.diffusion_step_encoder(timesteps)\n",
        "\n",
        "        if global_cond is not None:\n",
        "            global_feature = torch.cat([\n",
        "                global_feature, global_cond\n",
        "            ], axis=-1)\n",
        "        \n",
        "        x = sample\n",
        "        h = []\n",
        "        for idx, (resnet, resnet2, downsample) in enumerate(self.down_modules):\n",
        "            x = resnet(x, global_feature)\n",
        "            x = resnet2(x, global_feature)\n",
        "            h.append(x)\n",
        "            x = downsample(x)\n",
        "\n",
        "        for mid_module in self.mid_modules:\n",
        "            x = mid_module(x, global_feature)\n",
        "\n",
        "        for idx, (resnet, resnet2, upsample) in enumerate(self.up_modules):\n",
        "            x = torch.cat((x, h.pop()), dim=1)\n",
        "            x = resnet(x, global_feature)\n",
        "            x = resnet2(x, global_feature)\n",
        "            x = upsample(x)\n",
        "\n",
        "        x = self.final_conv(x)\n",
        "\n",
        "        # (B,C,T)\n",
        "        x = x.moveaxis(-1,-2)\n",
        "        # (B,T,C)\n",
        "        return x\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "X-XRB_g3vsgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Network Demo**\n",
        "\n",
        "# observation and action dimensions corrsponding to\n",
        "# the output of PushTEnv\n",
        "obs_dim = 5\n",
        "action_dim = 2\n",
        "\n",
        "# create network object\n",
        "noise_pred_net = ConditionalUnet1D(\n",
        "    input_dim=action_dim,\n",
        "    global_cond_dim=obs_dim*obs_horizon\n",
        ")\n",
        "\n",
        "# example inputs\n",
        "noised_action = torch.randn((1, pred_horizon, action_dim))\n",
        "obs = torch.zeros((1, obs_horizon, obs_dim))\n",
        "diffusion_iter = torch.zeros((1,))\n",
        "\n",
        "# the noise prediction network\n",
        "# takes noisy action, diffusion iteration and observation as input\n",
        "# predicts the noise added to action\n",
        "noise = noise_pred_net(\n",
        "    sample=noised_action, \n",
        "    timestep=diffusion_iter,\n",
        "    global_cond=obs.flatten(start_dim=1))\n",
        "\n",
        "# illustration of removing noise \n",
        "# the actual noise removal is performed by NoiseScheduler \n",
        "# and is dependent on the diffusion noise schedule\n",
        "denoised_action = noised_action - noise\n",
        "\n",
        "# for this demo, we use DDPMScheduler with 100 diffusion iterations\n",
        "num_diffusion_iters = 100\n",
        "noise_scheduler = DDPMScheduler(\n",
        "    num_train_timesteps=num_diffusion_iters,\n",
        "    # the choise of beta schedule has big impact on performance\n",
        "    # we found squared cosine works the best\n",
        "    beta_schedule='squaredcos_cap_v2',\n",
        "    # clip output to [-1,1] to improve stability\n",
        "    clip_sample=True,\n",
        "    # our network predicts noise (instead of denoised action)\n",
        "    prediction_type='epsilon'\n",
        ")\n",
        "\n",
        "# device transfer\n",
        "device = torch.device('cuda')\n",
        "_ = noise_pred_net.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4APZkqh336-M",
        "outputId": "ef5fd785-9be1-4e2d-f04f-bec3311c6857"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 6.535322e+07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Training**\n",
        "#@markdown\n",
        "#@markdown Takes about an hour. If you don't want to wait, skip to the next cell\n",
        "#@markdown to load pre-trained weights\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "# Exponential Moving Average\n",
        "# accelerates training and improves stability\n",
        "# holds a copy of the model weights\n",
        "ema = EMAModel(\n",
        "    model=noise_pred_net,\n",
        "    power=0.75)\n",
        "\n",
        "# Standard ADAM optimizer\n",
        "# Note that EMA parametesr are not optimized\n",
        "optimizer = torch.optim.AdamW(\n",
        "    params=noise_pred_net.parameters(), \n",
        "    lr=1e-4, weight_decay=1e-6)\n",
        "\n",
        "# Cosine LR schedule with linear warmup\n",
        "lr_scheduler = get_scheduler(\n",
        "    name='cosine',\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=500,\n",
        "    num_training_steps=len(dataloader) * num_epochs\n",
        ")\n",
        "\n",
        "with tqdm(range(num_epochs), desc='Epoch') as tglobal:\n",
        "    # epoch loop\n",
        "    for epoch_idx in tglobal:\n",
        "        epoch_loss = list()\n",
        "        # batch loop\n",
        "        with tqdm(dataloader, desc='Batch', leave=False) as tepoch:\n",
        "            for nbatch in tepoch:\n",
        "                # data normalized in dataset\n",
        "                # device transfer\n",
        "                nobs = nbatch['obs'].to(device)\n",
        "                naction = nbatch['action'].to(device)\n",
        "                B = nobs.shape[0]\n",
        "\n",
        "                # observation as FiLM conditioning\n",
        "                # (B, obs_horizon, obs_dim)\n",
        "                obs_cond = nobs[:,:obs_horizon,:]\n",
        "                # (B, obs_horizon * obs_dim)\n",
        "                obs_cond = obs_cond.flatten(start_dim=1)\n",
        "\n",
        "                # sample noise to add to actions\n",
        "                noise = torch.randn(naction.shape, device=device)\n",
        "\n",
        "                # sample a diffusion iteration for each data point\n",
        "                timesteps = torch.randint(\n",
        "                    0, noise_scheduler.config.num_train_timesteps, \n",
        "                    (B,), device=device\n",
        "                ).long()\n",
        "\n",
        "                # add noise to the clean images according to the noise magnitude at each diffusion iteration\n",
        "                # (this is the forward diffusion process)\n",
        "                noisy_actions = noise_scheduler.add_noise(\n",
        "                    naction, noise, timesteps)\n",
        "                \n",
        "                # predict the noise residual\n",
        "                noise_pred = noise_pred_net(\n",
        "                    noisy_actions, timesteps, global_cond=obs_cond)\n",
        "                \n",
        "                # L2 loss\n",
        "                loss = nn.functional.mse_loss(noise_pred, noise)\n",
        "\n",
        "                # optimize\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                # step lr scheduler every batch\n",
        "                # this is different from standard pytorch behavior\n",
        "                lr_scheduler.step()\n",
        "\n",
        "                # update Exponential Moving Average of the model weights\n",
        "                ema.step(noise_pred_net)\n",
        "\n",
        "                # logging\n",
        "                loss_cpu = loss.item()\n",
        "                epoch_loss.append(loss_cpu)\n",
        "                tepoch.set_postfix(loss=loss_cpu)\n",
        "        tglobal.set_postfix(loss=np.mean(epoch_loss))\n",
        "\n",
        "# Weights of the EMA model\n",
        "# is used for inference\n",
        "ema_noise_pred_net = ema.averaged_model"
      ],
      "metadata": {
        "id": "93E9RdnR4D8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Loading Pretrained Checkpoint**\n",
        "#@markdown Set `load_pretrained = True` to load pretrained weights.\n",
        "\n",
        "load_pretrained = False\n",
        "if load_pretrained:\n",
        "  ckpt_path = \"pusht_state_100ep.ckpt\"\n",
        "  if not os.path.isfile(ckpt_path):\n",
        "      id = \"1mHDr_DEZSdiGo9yecL50BBQYzR8Fjhl_&confirm=t\"\n",
        "      gdown.download(id=id, output=ckpt_path, quiet=False)\n",
        "\n",
        "  state_dict = torch.load(ckpt_path, map_location='cuda')\n",
        "  ema_noise_pred_net = noise_pred_net\n",
        "  ema_noise_pred_net.load_state_dict(state_dict)\n",
        "  print('Pretrained weights loaded.')\n",
        "else:\n",
        "  print(\"Skipped pretrained weight loading.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6F3hUbIuxGdO",
        "outputId": "f4e652ba-bea0-481d-e83f-8a6cd622a33a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1mHDr_DEZSdiGo9yecL50BBQYzR8Fjhl_&confirm=t\n",
            "To: /content/pusht_state_100ep.ckpt\n",
            "100%|██████████| 261M/261M [00:05<00:00, 45.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrained weights loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Inference**\n",
        "\n",
        "# limit enviornment interaction to 200 steps before termination\n",
        "max_steps = 200\n",
        "env = PushTEnv()\n",
        "# use a seed >200 to avoid initial states seen in the training dataset\n",
        "env.seed(100000)\n",
        "\n",
        "# get first observation\n",
        "obs = env.reset()\n",
        "\n",
        "# keep a queue of last 2 steps of observations\n",
        "obs_deque = collections.deque(\n",
        "    [obs] * obs_horizon, maxlen=obs_horizon)\n",
        "# save visualization and rewards\n",
        "imgs = [env.render(mode='rgb_array')]\n",
        "rewards = list()\n",
        "done = False\n",
        "step_idx = 0\n",
        "\n",
        "with tqdm(total=max_steps, desc=\"Eval PushTStateEnv\") as pbar:\n",
        "    while not done:\n",
        "        B = 1\n",
        "        # stack the last obs_horizon (2) number of observations\n",
        "        obs_seq = np.stack(obs_deque)\n",
        "        # normalize observation\n",
        "        nobs = normalize_data(obs_seq, stats=stats['obs'])\n",
        "        # device transfer\n",
        "        nobs = torch.from_numpy(nobs).to(device, dtype=torch.float32)\n",
        "\n",
        "        # infer action\n",
        "        with torch.no_grad():\n",
        "            # reshape observation to (B,obs_horizon*obs_dim)\n",
        "            obs_cond = nobs.unsqueeze(0).flatten(start_dim=1)\n",
        "\n",
        "            # initialize action from Guassian noise\n",
        "            noisy_action = torch.randn(\n",
        "                (B, pred_horizon, action_dim), device=device)\n",
        "            naction = noisy_action\n",
        "            \n",
        "            # init scheduler\n",
        "            noise_scheduler.set_timesteps(num_diffusion_iters)\n",
        "\n",
        "            for k in noise_scheduler.timesteps:\n",
        "                # predict noise\n",
        "                noise_pred = ema_noise_pred_net(\n",
        "                    sample=naction, \n",
        "                    timestep=k,\n",
        "                    global_cond=obs_cond\n",
        "                )\n",
        "\n",
        "                # inverse diffusion step (remove noise)\n",
        "                naction = noise_scheduler.step(\n",
        "                    model_output=noise_pred,\n",
        "                    timestep=k,\n",
        "                    sample=naction\n",
        "                ).prev_sample\n",
        "\n",
        "        # unnormalize action\n",
        "        naction = naction.detach().to('cpu').numpy()\n",
        "        # (B, pred_horizon, action_dim)\n",
        "        naction = naction[0]\n",
        "        action_pred = unnormalize_data(naction, stats=stats['action'])\n",
        "\n",
        "        # only take action_horizon number of actions\n",
        "        start = obs_horizon - 1\n",
        "        end = start + action_horizon\n",
        "        action = action_pred[start:end,:]\n",
        "        # (action_horizon, action_dim)\n",
        "\n",
        "        # execute action_horizon number of steps\n",
        "        # without replanning\n",
        "        for i in range(len(action)):\n",
        "            # stepping env\n",
        "            obs, reward, done, info = env.step(action[i])\n",
        "            # save observations\n",
        "            obs_deque.append(obs)\n",
        "            # and reward/vis\n",
        "            rewards.append(reward)\n",
        "            imgs.append(env.render(mode='rgb_array'))\n",
        "\n",
        "            # update progress bar\n",
        "            step_idx += 1\n",
        "            pbar.update(1)\n",
        "            pbar.set_postfix(reward=reward)\n",
        "            if step_idx > max_steps:\n",
        "                done = True\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "# print out the maximum target coverage\n",
        "print('Score: ', max(rewards))\n",
        "\n",
        "# visualize\n",
        "from IPython.display import Video\n",
        "vwrite('vis.mp4', imgs)\n",
        "Video('vis.mp4', embed=True, width=256, height=256)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348,
          "referenced_widgets": [
            "d3d29f28c9174a29a69857bc0b4a845a",
            "55d6b6d44b894986bd142a0e5d202278",
            "9110e03d3df7454d96a6faa6f17823a6",
            "61bfe1271ad943fb84cf0a2eb7098357",
            "b4da5d7c11194f9fa7313ddf9ec5dc4e",
            "2e055a762b714fd5b5d79804c585b3e3",
            "ef7033608015473587210584216fabc9",
            "4b52623ee9c143e29febe6b413469699",
            "aeaab85bac8c458fa05eb8944d17f94f",
            "19c8ff9b14ef45148165a5216d724fd3",
            "379f06dd98f344a28ccd761601865fc1"
          ]
        },
        "id": "OyLjlNQk5nr9",
        "outputId": "1f3d1044-02a5-405c-fa24-1a7ad0fb07c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval PushTStateEnv:   0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d3d29f28c9174a29a69857bc0b4a845a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score:  0.9803650015692196\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Video object>"
            ],
            "text/html": [
              "<video controls  width=\"256\"  height=\"256\">\n",
              " <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n",
              " Your browser does not support the video tag.\n",
              " </video>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}